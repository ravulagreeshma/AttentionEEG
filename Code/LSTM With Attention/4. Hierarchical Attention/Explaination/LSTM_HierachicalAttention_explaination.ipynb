{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Copy of LSTM_HierachicalAttention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noWTQ3PNsaYH"
      },
      "source": [
        "Hidden layer o/p will go as an input to the other layer"
      ],
      "id": "noWTQ3PNsaYH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbMLvjJQsmju"
      },
      "source": [
        "Hierarchical attention have stack of RNN Layers\n",
        "\n",
        "In one layer, everything happens, its o/p and hidden weights are passed to next layer\n",
        "\n",
        "Usually, In document classification, we have sentences and words in it\n",
        "\n",
        "So we need to understand words first. So in a word level we have attention\n",
        "\n",
        "On sentence level, we have one more attention \n",
        "\n",
        "On document level, we have another attention"
      ],
      "id": "HbMLvjJQsmju"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8IYJ3OSwvL5"
      },
      "source": [
        "For our EEG:\n",
        "\n",
        "We have two layers of attention\n",
        "First, we have first layer ka weight and its output.It is sent to second layer.\n"
      ],
      "id": "L8IYJ3OSwvL5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjpPUtU0lBqi"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import math"
      ],
      "id": "OjpPUtU0lBqi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNv0jGqvnheg",
        "outputId": "7c8de877-6dbd-4c55-a41c-543396cf5e93"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "id": "dNv0jGqvnheg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKBluhj9nsQD"
      },
      "source": [
        "path=\"/content/drive/My Drive/\""
      ],
      "id": "PKBluhj9nsQD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hkODO0vlBrV"
      },
      "source": [
        "### DataSet Description\n",
        "\n",
        "# 32 participants watched subset of 40 videos each (out of 120 videos) and these 32 participants ka brain signals and video are regarded and these ppl have rated the videos as well\n",
        "## we have 4 classes - arousal,valence,liking,dominance\n",
        "## so each .dat file has 40 videos(diff brain signals) ka binary info and for each of the signal there is a label associated\n",
        "## this is done across 40 channels - while pre-processing and downsampling freq-128Ghz and 60 sec trail period"
      ],
      "id": "3hkODO0vlBrV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAhrmayblBsm"
      },
      "source": [
        "class DeapS2SDatasetClassification(torch.utils.data.Dataset):\n",
        "    ''' This class is taking the path to the torch data as input and gives the processed data(in form of tensors) as output'''\n",
        "    def __init__(self, path):\n",
        "\n",
        "        _, _, filenames = next(os.walk(path))\n",
        "        filenames = sorted(filenames)\n",
        "        all_data = []\n",
        "        all_label = []\n",
        "        \n",
        "        ### opening the .datfiles and reading them and appending data and labels seperately\n",
        "        for dat in filenames:\n",
        "            temp = pickle.load(open(os.path.join(path,dat), 'rb'), encoding='latin1')\n",
        "\n",
        "            all_data.append(temp['data'])\n",
        "            all_label.append(temp['labels'][:,:2])\n",
        "\n",
        "        ### stacking the data so that this should be further converted to tensors to feed into models.\n",
        "        self.data = np.vstack(all_data)\n",
        "        self.label = np.vstack(all_label)\n",
        "        del temp, all_data, all_label\n",
        "\n",
        "    ## just getting the length of the data\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "    \n",
        "     #### Till this point we have each .dat file's data stacked,\n",
        "    # now with in each .dat file we have 40 samples,so we are seperating out each into single data and sinle label \n",
        "    ## and convering them to tensor and returning the final data\n",
        "    def __getitem__(self, idx):\n",
        "        single_data = self.data[idx]\n",
        "        single_label = self.label[idx].astype(float)\n",
        "        \n",
        "        batch = {\n",
        "            'data': torch.Tensor(single_data),\n",
        "            'label': torch.Tensor(single_label)\n",
        "        }\n",
        "\n",
        "        return batch"
      ],
      "id": "QAhrmayblBsm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNn3VtVI3k7A"
      },
      "source": [
        "def calculate_classification_metrics(pred,actual,best_class_weights):\n",
        "  acc = round(best_class_weights[0]*accuracy_score(np.vstack(pred).flatten(), np.vstack(actual).flatten()),3)\n",
        "  precision = round(best_class_weights[1]*precision_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='macro'),3)\n",
        "  recall = round(best_class_weights[2]*recall_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='macro'),3)\n",
        "  f1score = round(best_class_weights[3]*f1_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='macro'),3)\n",
        "  return acc,precision,recall,f1score"
      ],
      "id": "QNn3VtVI3k7A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRt1lKEqlBtG",
        "outputId": "c01beb69-77fc-4e75-ccc7-cedc12bf266a"
      },
      "source": [
        "## calling the above class here with our dataset path as input and here we are getting the entire data as output:- no.of samples here would be 1280 :- 40*32\n",
        "\n",
        "dataset = DeapS2SDatasetClassification(path+'data_preprocessed_python')\n",
        "\n",
        "## setting the seeed so that output doesn;t change each time we run the model\n",
        "torch.manual_seed(1)\n",
        "\n",
        "### doing the train and validation split \n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "## 80% data to training and rest 20% to validation\n",
        "train_ind = int(0.8 * len(dataset))\n",
        "\n",
        "## getting the train set out of whole data with the help of pytorch's subset method\n",
        "train_set = torch.utils.data.Subset(dataset, indices[:train_ind])\n",
        "\n",
        "## getting the val set with the help of pytorch's subset method\n",
        "val_set = torch.utils.data.Subset(dataset, indices[train_ind:])\n",
        "del dataset\n",
        "\n",
        "## checking the lenght of train and validation data,-> they should sum up to entire data(1280 samples)\n",
        "print(len(train_set))\n",
        "print(len(val_set))\n",
        "\n",
        "### Loading the data in form of torch data with batch size as 12,and shuffling the train set samples and similarly do it for val set and we don;t shuffle val set\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=12, shuffle=True, pin_memory=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=12, shuffle=False, pin_memory=True)\n",
        "\n"
      ],
      "id": "SRt1lKEqlBtG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024\n",
            "256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oGAj_gwlBt2"
      },
      "source": [
        "### defining the models and their architectures\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"this class will initialize the models with the desired architecture\"\"\"\n",
        "    def __init__(self, input_size, embed_size,\n",
        "                 n_layers=1, dropout=0.5):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        \n",
        "        ## defining lstm and it's embedding size and we are using bidirectional LSTM'S\n",
        "        self.lstm = nn.LSTM(input_size, embed_size, n_layers,\n",
        "                          dropout=dropout, bidirectional=True)\n",
        "    ## feed forward layer;s\n",
        "    def forward(self, x):\n",
        "        \n",
        "        output, (hn, cn) = self.lstm(x)\n",
        "        \n",
        "        # sum bidirectional outputs\n",
        "        output = (output[:, :, :self.embed_size] +\n",
        "                   output[:, :, self.embed_size:])\n",
        "        return output, hn"
      ],
      "id": "1oGAj_gwlBt2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmI7rILby-SE"
      },
      "source": [
        "This Layer1_attention is called in below class"
      ],
      "id": "BmI7rILby-SE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sox-1uViHZjW"
      },
      "source": [
        "class Layer1_Attention(nn.Module):\n",
        "    def __init__(self,output_size, hidden_dim, n_layers=1):\n",
        "        super(Layer1_Attention, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "#Encoderoutputs are sent into decoder as inputs to GRU\n",
        "        self.gru = nn.GRU(hidden_dim, hidden_dim, n_layers, batch_first=True, dropout=0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size).float()\n",
        "        self.tanh = nn.Tanh()\n",
        " \n",
        "      \n",
        "    def forward(self, x,hidden_dim):\n",
        "      #GRU gives output matrix and hidden weight matrix\n",
        "        out, h = self.gru(x)\n",
        "        #Sending the output to tanh function\n",
        "        out = self.fc(self.tanh(out))\n",
        "        return out\n",
        " #This is for weight matrix\n",
        " #Weights are been sent to next layer \n",
        "    def init_hidden(self, batch_size):\n",
        "      #Just weight matrix is kind of reshaping to be sent to next layer\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
        "        return hidden"
      ],
      "id": "Sox-1uViHZjW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbkteycr7RwI"
      },
      "source": [
        "So here instead of initialising weights randomly, from the above class out, hidden_weights are returned....\n",
        "\n",
        "out is taken as x here"
      ],
      "id": "rbkteycr7RwI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN-9MEaRstkt"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self,output_size, hidden_dim, n_layers=1):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.gru = nn.GRU(hidden_dim, hidden_dim, n_layers, batch_first=True, dropout=0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size).float()\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x,hidden_dim):\n",
        "        out, h = self.gru(x)\n",
        "        out = self.fc(self.relu(out))    \n",
        "        return out\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        # hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
        "        hidden=Layer1_Attention(weight,hidden_dim)   \n",
        "        return hidden"
      ],
      "id": "IN-9MEaRstkt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr3zdqcKlBuH"
      },
      "source": [
        "### Decoder class \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size,\n",
        "                 dropout=0.2):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        ##drop out  layer\n",
        "#         self.dropout = nn.Dropout(dropout, inplace=True)\n",
        "        \n",
        "        ## attention layer\n",
        "        self.attention = Attention(output_size,hidden_size)\n",
        "\n",
        "        ## linear - fully connected\n",
        "        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        \n",
        "        ## linear - fully connected\n",
        "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
        "        \n",
        "        ##Sigmoid \n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, last_hidden, encoder_outputs):\n",
        "\n",
        "        # Calculate attention weights and apply to encoder outputs\n",
        "        attn_weights = self.attention(encoder_outputs,last_hidden[-1])\n",
        "        # print(\"Attn\",attn_weights.shape)\n",
        "        # print(\"Attn Transpose\",attn_weights.transpose(1, 2).shape)\n",
        "\n",
        "        # print(\"Encoder\",encoder_outputs.shape)\n",
        "        # print(\"Encoder Transpose\",encoder_outputs.transpose(0, 1).shape)\n",
        "        context = attn_weights.transpose(1, 2).bmm(encoder_outputs)  # (B,1,N)\n",
        "        context = context.transpose(0, 1)  # (1,B,N)\n",
        "        output = self.fc(last_hidden.view(-1, 2*self.hidden_size))\n",
        "        context = context.squeeze(0)\n",
        "        # print(output.shape)\n",
        "        # print(context.shape)\n",
        "        # print(context[1:3].shape)\n",
        "        # context=torch.reshape(context, (16128, 256))\n",
        "        # print(\"After reshape\",context.shape)\n",
        "        output = self.out(torch.cat([output, output], 1))\n",
        "        #output = F.log_softmax(output, dim=1)\n",
        "        return self.sig(output), attn_weights"
      ],
      "id": "kr3zdqcKlBuH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGF9SJjqlBuv"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src):\n",
        "\n",
        "        encoder_output, hidden = self.encoder(src) \n",
        "        output, attn_weights = self.decoder(hidden, encoder_output)\n",
        "\n",
        "        return output"
      ],
      "id": "gGF9SJjqlBuv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm89i5XJlBv4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8484ead2-e236-420e-9503-19370d1c65f4"
      },
      "source": [
        "### getting the encoder layer with below units\n",
        "enc = Encoder(40, 256, 1).cuda()\n",
        "## getting the decoder layer\n",
        "dec = Decoder(256, 2).cuda()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "## connecting them with seq2seq and getting the final model out\n",
        "s2s = Seq2Seq(enc, dec).to(device)\n",
        "EPOCH = 15\n",
        "\n",
        "## binary cross entropy loss since our task is classification\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "## learning rate \n",
        "lr = 0.001\n",
        "\n",
        "\n",
        "\n",
        "## adam optimizer\n",
        "optimizer = torch.optim.AdamW(s2s.parameters(), lr=lr)"
      ],
      "id": "qm89i5XJlBv4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1MdID-mrlpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b92ce33-01ef-490e-bff2-30b5e959b612"
      },
      "source": [
        "## Training the model\n",
        "for epoch in range(15):\n",
        "    ## model.train\n",
        "    s2s.train()\n",
        "    train_loss = 0\n",
        "    \n",
        "    ## training bathces in gpu\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        data = batch['data'].permute(2, 0, 1).cuda()\n",
        "        label = batch['label'].cuda()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = s2s(data)\n",
        "        loss = loss_fn(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "\n",
        "    ## evaluating the trained model on validation set\n",
        "    s2s.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(val_loader):\n",
        "\n",
        "            data = batch['data'].permute(2, 0, 1).cuda()\n",
        "            label = batch['label'].cuda()\n",
        "            output = s2s(data)\n",
        "            loss = loss_fn(output, label)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print('Epoch : {} train_loss : {} val_loss : {}'.format(epoch, (opt_weight*train_loss)/len(train_loader), (opt_weight*val_loss)/len(val_loader)))       "
      ],
      "id": "G1MdID-mrlpI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0 train_loss : 0.2973962303032709 val_loss : 0.42850086558948863\n",
            "Epoch : 1 train_loss : 0.418269504635833 val_loss : 0.4294640267112039\n",
            "Epoch : 2 train_loss : 0.4188934194875318 val_loss : 0.4294640267112039\n",
            "Epoch : 3 train_loss : 0.4198124666435774 val_loss : 0.4294640267112039\n",
            "Epoch : 4 train_loss : 0.41889051534963206 val_loss : 0.4294640267112039\n",
            "Epoch : 5 train_loss : 0.41925423129769257 val_loss : 0.4294640267112039\n",
            "Epoch : 6 train_loss : 0.41902692643986195 val_loss : 0.4294640267112039\n",
            "Epoch : 7 train_loss : 0.4187335378957349 val_loss : 0.4294640267112039\n",
            "Epoch : 8 train_loss : 0.41909303354662514 val_loss : 0.4294640267112039\n",
            "Epoch : 9 train_loss : 0.4196337312210438 val_loss : 0.4294640267112039\n",
            "Epoch : 10 train_loss : 0.41872481696550234 val_loss : 0.4294640267112039\n",
            "Epoch : 11 train_loss : 0.41901938966263175 val_loss : 0.4294640267112039\n",
            "Epoch : 12 train_loss : 0.4190416781403298 val_loss : 0.4294640267112039\n",
            "Epoch : 13 train_loss : 0.41950485513376634 val_loss : 0.4294640267112039\n",
            "Epoch : 14 train_loss : 0.41939826504019806 val_loss : 0.4294640267112039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHaSQF9G-o9h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ecb84ae-e44a-454a-dd9e-f341eff478f0"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "### Calculating the metrics\n",
        "fin_targets = []\n",
        "fin_outputs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "        data = batch['data'].permute(2, 0, 1).cuda()\n",
        "        label = batch['label']\n",
        "        output = s2s(data)\n",
        "        fin_targets.append(np.asarray(label.numpy(),dtype=np.int))\n",
        "        fin_outputs.append(np.asarray((output.cpu().detach().numpy()>0.5), dtype=np.int))\n",
        "acc,precision,recall,f1score=calculate_classification_metrics(fin_outputs,fin_targets,best_class_weights)\n",
        "print('Accuracy : {}'.format(acc))\n",
        "print('Precision: {}'.format(precision))\n",
        "print('Recall: {}'.format(recall))\n",
        "print('F1score: {}'.format(f1score))"
      ],
      "id": "mHaSQF9G-o9h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.85\n",
            "Precision: 0.889\n",
            "Recall: 0.887\n",
            "F1score: 0.835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1416olKOqWKQ",
        "outputId": "8838e66d-0e8e-4a71-e0d0-65fc448f0be4"
      },
      "source": [
        "print(s2s)"
      ],
      "id": "1416olKOqWKQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (lstm): LSTM(40, 256, dropout=0.5, bidirectional=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (attention): Attention(\n",
            "      (gru): GRU(256, 256, batch_first=True, dropout=0.2)\n",
            "      (fc): Linear(in_features=256, out_features=2, bias=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (out): Linear(in_features=512, out_features=2, bias=True)\n",
            "    (sig): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}