{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1QnMx-LgpVH"
   },
   "source": [
    "HardAttention, hidden state is picked randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EN35joxR2Li8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s7ZCPbBWDvvv",
    "outputId": "69b12244-1564-4806-da2b-67e233fcbe68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9G5IT_6D8Tk"
   },
   "outputs": [],
   "source": [
    "path=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3r38g3_Jx9y7",
    "outputId": "dba9e6f8-02aa-41f0-92c0-4336c99d7ca6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [2,34,5,76,8,94,3,2]\n",
    "a[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orYIhi9DEB_O"
   },
   "outputs": [],
   "source": [
    "class DeapS2SDatasetClassification(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "\n",
    "        _, _, filenames = next(os.walk(path))\n",
    "        filenames = sorted(filenames)\n",
    "        all_data = []\n",
    "        all_label = []\n",
    "        for dat in filenames:\n",
    "            temp = pickle.load(open(os.path.join(path,dat), 'rb'), encoding='latin1')\n",
    "            all_data.append(temp['data'])\n",
    "            all_label.append(temp['labels'][:,:1]) #For valance & #For arousal [:,1:2]\n",
    "        \n",
    "        self.data = np.vstack(all_data)\n",
    "        self.label = np.vstack(all_label)\n",
    "        del temp, all_data, all_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        single_data = self.data[idx]\n",
    "        single_label = (self.label[idx] > 5).astype(float)\n",
    "        \n",
    "        batch = {\n",
    "            'data': torch.Tensor(single_data),\n",
    "            'label': torch.Tensor(single_label)\n",
    "        }\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMLk2p3TEHh3"
   },
   "outputs": [],
   "source": [
    "def classification_report(pred,actual,best_class_weights):\n",
    "    acc = round(best_class_weights[0]*(accuracy_score(np.vstack(pred).flatten(), np.vstack(actual).flatten())),2)\n",
    "    precision = round(best_class_weights[1]*precision_score(np.vstack(pred).flatten(), np.vstack(actual).flatten()),2)\n",
    "    recall = round(best_class_weights[1]*recall_score(np.vstack(pred).flatten(), np.vstack(actual).flatten()),2)\n",
    "    f1score = round(best_class_weights[1]*f1_score(np.vstack(pred).flatten(), np.vstack(actual).flatten()),2)\n",
    "    return acc,precision,recall,f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JBBTPveeEJhK",
    "outputId": "93f5bf65-b6d7-4ad7-b660-680507ec1733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "dataset = DeapS2SDatasetClassification(path+'data_preprocessed_python')\n",
    "\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_ind = int(0.7 * len(dataset))\n",
    "train_set = torch.utils.data.Subset(dataset, indices[:train_ind])\n",
    "val_set = torch.utils.data.Subset(dataset, indices[train_ind:])\n",
    "del dataset\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(val_set))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHlQLBzoEvRJ"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "#initializing random hidden size\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        \n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "      #Length of encoder o/p\n",
    "        timestep = encoder_outputs.size(0)\n",
    "\n",
    "      #we have repeate the length of hidden unit\n",
    "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
    "\n",
    "      #transposing\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  \n",
    "  \n",
    "  #cat (enc_o/p,hidden_states)\n",
    "        temp = torch.cat([h, encoder_outputs], dim=2)\n",
    "  #applying linear layer, relu activation fun to calculate attention weights\n",
    "        energy = F.relu(self.attn(temp))\n",
    "  #reshaping\n",
    "        energy = energy.transpose(1, 2) \n",
    "  #Since hidden states are to be picked random, so we are applying v\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1) \n",
    "  #multiplying with energies \n",
    "        energy = torch.bmm(v, energy)\n",
    "        attn_energies = energy.squeeze(1)\n",
    "  #applying siftmax_fun\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fd5HBmIpE0hH"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size,\n",
    "                 n_layers=1, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm = nn.LSTM(input_size, embed_size, n_layers,\n",
    "                          dropout=dropout, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        output = (output[:, :, :self.embed_size] +\n",
    "                   output[:, :, self.embed_size:])\n",
    "        return output, hn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Uv7BT7XE55k"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size,\n",
    "                 dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        self.attention = Attention(hidden_size)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, last_hidden, encoder_outputs):\n",
    "        attn_weights = self.attention(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  \n",
    "        context = context.transpose(0, 1)  \n",
    "        output = self.fc(last_hidden.view(-1, 2*self.hidden_size))\n",
    "        context = context.squeeze(0)\n",
    "        output = self.out(torch.cat([output, context], 1))\n",
    "        return self.sig(output), attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gz90rMoGE848"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src):\n",
    "\n",
    "        encoder_output, hidden = self.encoder(src) \n",
    "        output, attn_weights = self.decoder(hidden, encoder_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJmhKJr1FAe8",
    "outputId": "c188b5df-4ddf-4218-bb6b-81cf892a4833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 40, 8064])\n",
      "torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "data = dataiter.next()\n",
    "images, labels = data['data'],data['label']\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "amg78ZE4FKZi",
    "outputId": "083c17ef-df4d-404c-ceaa-54273622567a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shashank\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "enc = Encoder(40, 128, 1).cuda()\n",
    "dec = Decoder(128, 1).cuda()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "s2s = Seq2Seq(enc, dec).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "lr = 0.01\n",
    "best_class_weights=[1.5,1.35]\n",
    "optimizer = torch.optim.AdamW(s2s.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q0QFxTjoFQsz",
    "outputId": "626d7b02-658a-4d39-8fb8-525511551bf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (lstm): LSTM(40, 128, dropout=0.5, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (dropout): Dropout(p=0.2, inplace=True)\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (out): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (sig): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(s2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RImVunlSFSxF",
    "outputId": "03efbb8d-b94d-420a-f4bf-b992ce496c0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 train_loss : 0.7893407009541988 val_loss : 0.7988337824741999\n",
      "Epoch : 1 train_loss : 0.7935695871710777 val_loss : 0.8968918398022652\n",
      "Epoch : 2 train_loss : 0.79569261521101 val_loss : 0.7766000057260195\n",
      "Epoch : 3 train_loss : 0.7734775245189667 val_loss : 0.7624355504910151\n",
      "Epoch : 4 train_loss : 0.7652316870433944 val_loss : 0.8154181366165479\n",
      "Epoch : 5 train_loss : 0.7467129049556596 val_loss : 0.7559524402022362\n",
      "Epoch : 6 train_loss : 0.742796601993697 val_loss : 0.800188864270846\n",
      "Epoch : 7 train_loss : 0.7714240779834134 val_loss : 0.7859725095331669\n",
      "Epoch : 8 train_loss : 0.7289992576198918 val_loss : 0.8263038446505865\n",
      "Epoch : 9 train_loss : 0.7442753192569528 val_loss : 0.80012222006917\n",
      "Epoch : 10 train_loss : 0.7732076474598476 val_loss : 0.8240370365480582\n",
      "Epoch : 11 train_loss : 0.7714540798749242 val_loss : 0.8957976425687472\n",
      "Epoch : 12 train_loss : 0.7664059230259487 val_loss : 0.8049847570558389\n",
      "Epoch : 13 train_loss : 0.7691642461078507 val_loss : 0.8433674524227778\n",
      "Epoch : 14 train_loss : 0.7785752027162484 val_loss : 0.8834269270300865\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "    s2s.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        data = batch['data'].permute(2, 0, 1).cuda()\n",
    "        label = batch['label'].cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = s2s(data)\n",
    "        loss = loss_fn(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    s2s.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "\n",
    "            data = batch['data'].permute(2, 0, 1).cuda()\n",
    "            label = batch['label'].cuda()\n",
    "            output = s2s(data)\n",
    "            loss = loss_fn(output, label)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print('Epoch : {} train_loss : {} val_loss : {}'.format(epoch, train_loss/len(train_loader), val_loss/len(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqS0SmzIKMwU",
    "outputId": "778b0981-35d6-43b1-b4d8-c9d08489bf56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.85\n",
      "Precision: 0.75\n",
      "Recall: 0.85\n",
      "F1score: 0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "fin_targets = []\n",
    "fin_outputs = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "        data = batch['data'].permute(2, 0, 1).cuda()\n",
    "        label = batch['label']\n",
    "        output = s2s(data)\n",
    "        fin_targets.append(label.numpy())\n",
    "        fin_outputs.append(np.asarray((output.cpu().detach().numpy()>0.5), dtype=np.int))\n",
    "        \n",
    "acc,precision,recall,f1score = classification_report(fin_outputs,fin_targets,best_class_weights)\n",
    "\n",
    "print('Accuracy : {}'.format(acc))\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1score: {}'.format(f1score))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final_hardattention_valence .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
