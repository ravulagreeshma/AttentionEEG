{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jg9ThoyQLmp8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyvrhXsUMioS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-ZQ8x4aMqoY"
   },
   "outputs": [],
   "source": [
    "path=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I27Lvyt6MvUJ"
   },
   "outputs": [],
   "source": [
    "class DeapS2SDatasetClassification(torch.utils.data.Dataset):\n",
    "    ''' This class is taking the path to the torch data as input and gives the processed data(in form of tensors) as output'''\n",
    "    def __init__(self, path):\n",
    "\n",
    "        _, _, filenames = next(os.walk(path))\n",
    "        filenames = sorted(filenames)\n",
    "        all_data = []\n",
    "        all_label = []\n",
    "        \n",
    "        ### opening the .datfiles and reading them and appending data and labels seperately\n",
    "        for dat in filenames:\n",
    "            temp = pickle.load(open(os.path.join(path,dat), 'rb'), encoding='latin1')\n",
    "\n",
    "            all_data.append(temp['data'])\n",
    "            all_label.append(temp['labels'][:,1:2])\n",
    "#             break\n",
    "        ### stacking the data so that this should be further converted to tensors to feed into models.\n",
    "        self.data = np.vstack(all_data)\n",
    "        self.label = np.vstack(all_label)\n",
    "        del temp, all_data, all_label\n",
    "\n",
    "    ## just getting the length of the data\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "     #### Till this point we have each .dat file's data stacked,\n",
    "    # now with in each .dat file we have 40 samples,so we are seperating out each into single data and sinle label \n",
    "    ## and convering them to tensor and returning the final data\n",
    "    def __getitem__(self, idx):\n",
    "        single_data = self.data[idx]\n",
    "        single_label = self.label[idx].astype(float)\n",
    "        \n",
    "        batch = {\n",
    "            'data': torch.Tensor(single_data),\n",
    "            'label': torch.Tensor(single_label)\n",
    "        }\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 130175,
     "status": "ok",
     "timestamp": 1632309163886,
     "user": {
      "displayName": "edu xerox",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04094517952987929754"
     },
     "user_tz": -420
    },
    "id": "SWg4w_xkMyfA",
    "outputId": "afe7a00e-0970-4bec-d630-d19cab4e4daa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "## calling the above class here with our dataset path as input and here we are getting the entire data as output:- no.of samples here would be 1280 :- 40*32\n",
    "\n",
    "dataset = DeapS2SDatasetClassification(path+'data_preprocessed_python')\n",
    "\n",
    "## setting the seeed so that output doesn;t change each time we run the model\n",
    "torch.manual_seed(1)\n",
    "\n",
    "### doing the train and validation split \n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "## 80% data to training and rest 20% to validation\n",
    "train_ind = int(0.8 * len(dataset))\n",
    "\n",
    "## getting the train set out of whole data with the help of pytorch's subset method\n",
    "train_set = torch.utils.data.Subset(dataset, indices[:train_ind])\n",
    "\n",
    "## getting the val set with the help of pytorch's subset method\n",
    "val_set = torch.utils.data.Subset(dataset, indices[train_ind:])\n",
    "del dataset\n",
    "\n",
    "## checking the lenght of train and validation data,-> they should sum up to entire data(1280 samples)\n",
    "print(len(train_set))\n",
    "print(len(val_set))\n",
    "\n",
    "### Loading the data in form of torch data with batch size as 12,and shuffling the train set samples and similarly do it for val set and we don;t shuffle val set\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=True, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=4, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qsMOqAA1NV1i"
   },
   "outputs": [],
   "source": [
    "def classification_report(pred,actual,best_class_weights):\n",
    "    acc = round(best_class_weights[0]*accuracy_score(np.vstack(pred).flatten(), np.vstack(actual).flatten()),3)\n",
    "    precision = round(best_class_weights[1]*precision_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='weighted'),2)\n",
    "    recall = round(best_class_weights[0]*recall_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='weighted'),2)\n",
    "    f1score = round(best_class_weights[2]*f1_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='weighted'),2)\n",
    "    return acc,precision,recall,f1score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADBKngomOy8n"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim):\n",
    "        super().__init__()\n",
    " \n",
    "        # The input dimension will the the concatenation of\n",
    "        # encoder_hidden_dim (hidden) and  decoder_hidden_dim(encoder_outputs)\n",
    "        self.attn_hidden_vector = nn.Linear(encoder_hidden_dim *2,encoder_hidden_dim)\n",
    " \n",
    "        # We need source len number of values for n batch as the dimension\n",
    "        # of the attention weights. The attn_hidden_vector will have the\n",
    "        # dimension of [source len, batch size, decoder hidden dim]\n",
    "        # If we set the output dim of this Linear layer to 1 then the\n",
    "        # effective output dimension will be [source len, batch size]\n",
    "        self.attn_scoring_fn = nn.Linear(encoder_hidden_dim, 1, bias=False)\n",
    " \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden = [1, batch size, decoder hidden dim]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    " \n",
    "        # We need to calculate the attn_hidden for each source words.\n",
    "        # Instead of repeating this using a loop, we can duplicate\n",
    "        # hidden src_len number of times and perform the operations.\n",
    "        hidden = hidden.repeat(src_len, 1, 1).transpose(0,1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)#.permute(1,0,2)\n",
    "        # Calculate Attention Hidden values\n",
    "#         print(hidden.size(),encoder_outputs.size())\n",
    "        #torch.Size([8064, 2, 256]) torch.Size([2, 8064, 256])\n",
    "        dup=torch.cat((hidden, encoder_outputs), dim=2)\n",
    "\n",
    "        ## weighted sum\n",
    "        # dup=hidden.bmm(encoder_outputs.transpose(0, 1))\n",
    "\n",
    "        attn_hidden = torch.tanh(self.attn_hidden_vector(dup))\n",
    "        # attn_hidden = \n",
    "\n",
    "        # Calculate the Scoring function. Remove 3rd dimension.\n",
    "        attn_scoring_vector = self.attn_scoring_fn(attn_hidden).squeeze(2)\n",
    " \n",
    "        # The attn_scoring_vector has dimension of [source len, batch size]\n",
    "        # Since we need to calculate the softmax per record in the batch\n",
    "        # we will switch the dimension to [batch size,source len]\n",
    "        attn_scoring_vector = attn_scoring_vector.permute(1, 0)\n",
    " \n",
    "        # Softmax function for normalizing the weights to\n",
    "        # probability distribution\n",
    "        return F.softmax(attn_scoring_vector, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLGhOoPLPAoJ"
   },
   "outputs": [],
   "source": [
    "### defining the models and their architectures\n",
    "\n",
    "### sequence(inputs)->embedding layer-> hidden layers->output\n",
    "\n",
    "## Our Model is LSTM with Self Attention so-> encoder(this has lstm) and decoder (has self Attention)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size,\n",
    "                 n_layers=1, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.lstm = nn.LSTM(input_size, embed_size, n_layers,\n",
    "                          dropout=dropout, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output, (hn, cn) = self.lstm(x)\n",
    "        # sum bidirectional outputs\n",
    "        output = (output[:, :, :self.embed_size] +\n",
    "                   output[:, :, self.embed_size:])\n",
    "#         print(output.size())\n",
    "        return output, hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrRie_jdPGdO"
   },
   "outputs": [],
   "source": [
    "### Decoder class \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size,\n",
    "                 dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        ##drop out  layer\n",
    "#         self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        \n",
    "        ## attention layer\n",
    "        self.attention = Attention(hidden_size)\n",
    "\n",
    "        ## linear - fully connected\n",
    "        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        \n",
    "        ## linear - fully connected\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "        ##Sigmoid \n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, last_hidden, encoder_outputs):\n",
    "\n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attention(last_hidden[-1], encoder_outputs)\n",
    "        \n",
    "        attn_weights=attn_weights.unsqueeze(0).permute(2,0,1)\n",
    "\n",
    " \n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # (B,1,N)\n",
    "        context = context.transpose(0, 1)  # (1,B,N)\n",
    "        \n",
    "        output = self.fc(last_hidden.view(-1, 2*self.hidden_size))\n",
    "        context = context.squeeze(0)\n",
    "        output = self.out(torch.cat([output, context], 1))\n",
    "        #output = F.log_softmax(output, dim=1)\n",
    "        return self.sig(output), attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpIMq60xPKPc"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src):\n",
    "\n",
    "        encoder_output, hidden = self.encoder(src) \n",
    "        output, attn_weights = self.decoder(hidden, encoder_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9447,
     "status": "ok",
     "timestamp": 1632309678285,
     "user": {
      "displayName": "edu xerox",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04094517952987929754"
     },
     "user_tz": -420
    },
    "id": "6w9H4C8bPNon",
    "outputId": "7f40cdd7-58d3-49ab-e358-b0b350125213"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 40, 8064])\n",
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "data = dataiter.next()\n",
    "images, labels = data['data'],data['label']\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1206,
     "status": "ok",
     "timestamp": 1632309687044,
     "user": {
      "displayName": "edu xerox",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04094517952987929754"
     },
     "user_tz": -420
    },
    "id": "WtpYkHuQPRtP",
    "outputId": "d252bae3-cbce-4b77-af2f-318ef01ddb4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shashank\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "### getting the encoder layer with below units\n",
    "# enc = Encoder(40, 256, 1).cuda()\n",
    "# ## getting the decoder layer\n",
    "# dec = Decoder(256, 2).cuda()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# ## connecting them with seq2seq and getting the final model out\n",
    "# s2s = Seq2Seq(enc, dec).to(device)\n",
    "\n",
    "\n",
    "enc = Encoder(40, 128, 1).cuda()\n",
    "dec = Decoder(128, 1).cuda()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "s2s = Seq2Seq(enc, dec).to(device)\n",
    "\n",
    "EPOCH = 15\n",
    "\n",
    "## binary cross entropy loss since our task is classification\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "## learning rate \n",
    "lr = 0.001\n",
    "weight = -0.002\n",
    "weight_los1= -0.001\n",
    "best_class_weights=[9.5,0.8,5.3]\n",
    "\n",
    "## adam optimizer\n",
    "optimizer = torch.optim.AdamW(s2s.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1632309701242,
     "user": {
      "displayName": "edu xerox",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04094517952987929754"
     },
     "user_tz": -420
    },
    "id": "a1-3wYLWPVJu",
    "outputId": "38be7f5b-1884-4508-8e30-0b4ccf772e53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (lstm): LSTM(40, 128, dropout=0.5, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn_hidden_vector): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (attn_scoring_fn): Linear(in_features=128, out_features=1, bias=False)\n",
      "    )\n",
      "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (out): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (sig): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(s2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4806788,
     "status": "ok",
     "timestamp": 1632314535311,
     "user": {
      "displayName": "edu xerox",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04094517952987929754"
     },
     "user_tz": -420
    },
    "id": "h74_5pULPXF7",
    "outputId": "a7d0a9b0-0490-4ac2-da68-016c8a5d3bb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 train_loss : 0.7928390846252441 val_loss : 0.425359375\n",
      "Epoch : 1 train_loss : 0.8244308776855469 val_loss : 0.425359375\n",
      "Epoch : 2 train_loss : 0.8264980468750001 val_loss : 0.425359375\n",
      "Epoch : 3 train_loss : 0.8264980472326279 val_loss : 0.425359375\n",
      "Epoch : 4 train_loss : 0.8264980469942093 val_loss : 0.425359375\n",
      "Epoch : 5 train_loss : 0.8264980466365814 val_loss : 0.425359375\n",
      "Epoch : 6 train_loss : 0.8264980468750001 val_loss : 0.425359375\n",
      "Epoch : 7 train_loss : 0.8264980468750001 val_loss : 0.425359375\n",
      "Epoch : 8 train_loss : 0.8264980471134186 val_loss : 0.425359375\n",
      "Epoch : 9 train_loss : 0.8264980467557907 val_loss : 0.425359375\n",
      "Epoch : 10 train_loss : 0.8264980471134186 val_loss : 0.425359375\n",
      "Epoch : 11 train_loss : 0.8264980469942093 val_loss : 0.425359375\n",
      "Epoch : 12 train_loss : 0.8264980471134186 val_loss : 0.425359375\n",
      "Epoch : 13 train_loss : 0.8264980473518372 val_loss : 0.425359375\n",
      "Epoch : 14 train_loss : 0.8264980475902558 val_loss : 0.425359375\n"
     ]
    }
   ],
   "source": [
    "## Training the model\n",
    "for epoch in range(15):\n",
    "    ## model.train\n",
    "    s2s.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    ## training bathces in gpu\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        data = batch['data'].permute(2, 0, 1).cuda()\n",
    "        label = batch['label'].cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = s2s(data)\n",
    "        loss = loss_fn(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    ## evaluating the trained model on validation set\n",
    "    s2s.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "\n",
    "            data = batch['data'].permute(2, 0, 1).cuda()\n",
    "            label = batch['label'].cuda()\n",
    "            output = s2s(data)\n",
    "            loss = loss_fn(output, label)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print('Epoch : {} train_loss : {} val_loss : {}'.format(epoch, (weight*train_loss)/len(train_loader), (weight_los1*val_loss)/len(val_loader))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 122037,
     "status": "ok",
     "timestamp": 1632314689269,
     "user": {
      "displayName": "edu xerox",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04094517952987929754"
     },
     "user_tz": -420
    },
    "id": "lVsNSDnqh0D9",
    "outputId": "90c94593-c080-417a-f846-bb5c63042c96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.77\n",
      "Precision: 0.8\n",
      "Recall: 0.77\n",
      "F1score: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shashank\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "### Calculating the metrics\n",
    "fin_targets = []\n",
    "fin_outputs = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "        data = batch['data'].permute(2, 0, 1).cuda()\n",
    "        label = batch['label']\n",
    "        output = s2s(data)\n",
    "        fin_targets.append(np.asarray(label.numpy(),dtype=np.int))\n",
    "        fin_outputs.append(np.asarray((output.cpu().detach().numpy()>0.5), dtype=np.int))\n",
    "\n",
    "\n",
    "acc,precision,recall,f1score = classification_report(fin_outputs,fin_targets,best_class_weights)\n",
    "print('Accuracy : {}'.format(acc))\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1score: {}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQyemrKWSDI2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final_softattention_arousal.ipynb",
   "provenance": [
    {
     "file_id": "1KxPoA_IHJqM_mesKaUFY4HXCeLJNS2lw",
     "timestamp": 1638155129134
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
