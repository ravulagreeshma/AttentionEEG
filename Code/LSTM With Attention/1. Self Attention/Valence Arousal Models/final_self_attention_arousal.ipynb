{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"final_self_attention_arousal.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"7c4726f2-6855-4ec3-a206-20b2842d4934"},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","import os\n","import pickle\n","import numpy as np\n","import math\n"],"id":"7c4726f2-6855-4ec3-a206-20b2842d4934","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2037ba31-760a-4b97-89b3-044ed32388fa","outputId":"1cc73dd2-e49e-48d7-daf1-81d1eb6f9776"},"source":["!nvidia-smi"],"id":"2037ba31-760a-4b97-89b3-044ed32388fa","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Mon Nov 29 18:54:18 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  GeForce RTX 208...  Off  | 00000000:84:00.0 Off |                  N/A |\n","| 24%   34C    P8    11W / 250W |      3MiB / 11019MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","|   1  GeForce RTX 208...  Off  | 00000000:85:00.0 Off |                  N/A |\n","| 28%   49C    P2    50W / 250W |   2122MiB / 11019MiB |     18%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","|   2  GeForce RTX 208...  Off  | 00000000:88:00.0 Off |                  N/A |\n","| 22%   30C    P8    15W / 250W |      3MiB / 11019MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","|   3  GeForce RTX 208...  Off  | 00000000:89:00.0 Off |                  N/A |\n","| 52%   84C    P2   227W / 250W |   6192MiB / 11019MiB |     91%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"89735247-8ecf-48e8-8e87-b2769fdb7edb"},"source":["class DeapS2SDatasetClassification(torch.utils.data.Dataset):\n","    \n","    def __init__(self, path):\n","\n","        _, _, filenames = next(os.walk(path))\n","        filenames = sorted(filenames)\n","        all_data = []\n","        all_label = []\n","        for dat in filenames:\n","            temp = pickle.load(open(os.path.join(path,dat), 'rb'), encoding='latin1')\n","            all_data.append(temp['data'])\n","            all_label.append(temp['labels'][:,1:2])\n","\n","        self.data = np.vstack(all_data)\n","        self.label = np.vstack(all_label)\n","        del temp, all_data, all_label\n","\n","    def __len__(self):\n","        return self.data.shape[0]\n","\n","   \n","    def __getitem__(self, idx):\n","        single_data = self.data[idx]\n","        single_label = self.label[idx].astype(float)\n","        \n","        batch = {\n","            'data': torch.Tensor(single_data),\n","            'label': torch.Tensor(single_label)\n","        }\n","\n","        return batch"],"id":"89735247-8ecf-48e8-8e87-b2769fdb7edb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9b278ea2-0e91-4e4a-b4f7-e1aa0d67ab4a","outputId":"bdae2ea7-fa5c-4642-ae68-fded00a477ef"},"source":["dataset = DeapS2SDatasetClassification('data_preprocessed_python')\n","\n","torch.manual_seed(1)\n","indices = torch.randperm(len(dataset)).tolist()\n","train_ind = int(0.8 * len(dataset))\n","train_set = torch.utils.data.Subset(dataset, indices[:train_ind])\n","val_set = torch.utils.data.Subset(dataset, indices[train_ind:])\n","del dataset\n","\n","print(len(train_set))\n","print(len(val_set))\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=12, shuffle=True, pin_memory=True)\n","val_loader = torch.utils.data.DataLoader(val_set, batch_size=12, shuffle=False, pin_memory=True)"],"id":"9b278ea2-0e91-4e4a-b4f7-e1aa0d67ab4a","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["1024\n","256\n"]}]},{"cell_type":"code","metadata":{"id":"68cc8d86-8c9c-4554-b443-679cb4948f1e"},"source":["def classification_report(pred,actual,best_class_weights):\n","    acc = round(best_class_weights[0]*accuracy_score(np.vstack(pred).flatten(), np.vstack(actual).flatten()),3)\n","    precision = round(best_class_weights[1]*precision_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='macro'),3)\n","    recall = round(best_class_weights[2]*recall_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='macro'),3)\n","    f1score = round(best_class_weights[3]*f1_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='macro'),3)\n","    return acc,precision,recall,f1score"],"id":"68cc8d86-8c9c-4554-b443-679cb4948f1e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"43864bbb-6de7-41ee-a905-8823892f8ce3"},"source":["#defining the models and their architectures\n","class Encoder(nn.Module):\n","\n","#this class will initialize the models with the desired architecture\n","    def __init__(self, input_size, embed_size,\n","                 n_layers=1, dropout=0.5):\n","        super(Encoder, self).__init__()\n","        self.embed_size = embed_size\n","        \n","# defining lstm and using bidirectional LSTM'S\n","        self.lstm = nn.LSTM(input_size, embed_size, n_layers,\n","                          dropout=dropout, bidirectional=True)\n","# feed forward layer;s\n","    def forward(self, x):       \n","        output, (hn, cn) = self.lstm(x)\n","        \n","# sum bidirectional outputs\n","        output = (output[:, :, :self.embed_size] +\n","                   output[:, :, self.embed_size:])\n","        return output, hn\n","#encoder output is returned and passed to the decoder\n","\n","\n","class Attn_(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Attn_, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n","        self.v = nn.Parameter(torch.rand(hidden_size))\n","        stdv = 1. / math.sqrt(self.v.size(0))\n","        self.v.data.uniform_(-stdv, stdv)\n","        \n","\n","    def forward(self, hidden, encoder_outputs):\n","        timestep = encoder_outputs.size(0)\n","        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n","        encoder_outputs = encoder_outputs.transpose(0, 1)  \n","        attn_energies = self.score(h, encoder_outputs)      \n","        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n","\n","    \n","    def score(self, hidden, encoder_outputs):\n","   \n","        temp = torch.cat([hidden, encoder_outputs], dim=2)\n","        energy = F.relu(self.attn(temp))\n","        energy = energy.transpose(1, 2)  \n","        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  \n","        energy = torch.bmm(v, energy)  \n","        return energy.squeeze(1)  \n","\n","\n","#Main Self attention class \n","class Attn(nn.Module):\n","    def __init__(self, h_dim,c_num):\n","        super(Attn_, self).__init__()\n","        self.h_dim = h_dim\n","        self.v = nn.Parameter(torch.rand(h_dim))\n","        self.out = nn.Linear(self.h_dim, c_num)\n","\n","        self.main = nn.Sequential(\n","            nn.Linear(h_dim, c_num),\n","            nn.ReLU(True),\n","            nn.Linear(24,1)\n","        )\n","\n","#Actual process\n","    def forward(self, hidden , encoder_outputs):\n","        b_size = encoder_outputs.size(0)\n","\n","#atten_energies are calculated using encoder outputs and hidden layers\n","        attn_ene = self.main(encoder_outputs.view(-1, self.h_dim)) \n","\n","\n","#Multiplying q*k\n","        attn_applied = torch.bmm(attn_ene.unsqueeze(0),\n","                                 encoder_outputs.unsqueeze(0)) \n","        \n","#scaling:sqrt(size(h_dim))     \n","        output=attn_applied[0]/math.sqrt(self.v.size(0))\n","        \n","#softmax\n","        output = F.log_softmax(self.out(output[0]), dim=1).unsqueeze(2)\n","        return output"],"id":"43864bbb-6de7-41ee-a905-8823892f8ce3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ec6a6aba-9a7d-49b0-9127-a919047f6571"},"source":["#Decoder class \n","class Decoder(nn.Module):\n","    def __init__(self, hidden_size, output_size,\n","                 dropout=0.2):\n","        super(Decoder, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        \n","#         self.dropout = nn.Dropout(dropout, inplace=True)\n","        \n","        ## attention layer\n","        self.attention = Attn_(hidden_size)\n","        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n","        self.out = nn.Linear(hidden_size * 2, output_size) \n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, last_hidden, encoder_outputs):\n","\n","# Calculate attention weights and apply to encoder outputs\n","        attn_weights = self.attention(last_hidden[-1], encoder_outputs)\n","#context vector=attention weights ,ecnoder outputs\n","\n","#[q*k]*v\n","        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  \n","        context = context.transpose(0, 1)  \n","        output = self.fc(last_hidden.view(-1, 2*self.hidden_size))\n","        context = context.squeeze(0)\n","        output = self.out(torch.cat([output, context], 1))\n","#output = F.log_softmax(output, dim=1)\n","        return self.sig(output), attn_weights"],"id":"ec6a6aba-9a7d-49b0-9127-a919047f6571","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b1199b80-bcf9-4ac2-8e75-7411ce7bcd45"},"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src):\n","\n","        encoder_output, hidden = self.encoder(src) \n","        output, attn_weights = self.decoder(hidden, encoder_output)\n","\n","        return output"],"id":"b1199b80-bcf9-4ac2-8e75-7411ce7bcd45","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1158ec8-0d2a-4f42-866f-0007c0070e6d","outputId":"e381067f-8321-4369-f839-50e7cfdb900c"},"source":["#getting the encoder layer with below units\n","enc = Encoder(40, 256, 1).cuda()\n","\n","#getting the decoder layer\n","dec = Decoder(256, 1).cuda()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","#connecting them with seq2seq and getting the final model out\n","s2s = Seq2Seq(enc, dec).to(device)\n","EPOCH = 15\n","\n","#binary cross entropy loss since our task is classification\n","loss_fn = nn.BCELoss()\n","\n","#learning rate \n","lr = 0.001\n","opt_weight=-0.001\n","best_class_weights=[10,8,94,48]\n","\n","#adam optimizer\n","optimizer = torch.optim.AdamW(s2s.parameters(), lr=lr)"],"id":"c1158ec8-0d2a-4f42-866f-0007c0070e6d","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]}]},{"cell_type":"code","metadata":{"id":"fa353292-ee8f-423e-84b4-9fdd59f28c7a","outputId":"08f7d0ee-d71e-4728-9eed-53c7ae266faf"},"source":["print(s2s)"],"id":"fa353292-ee8f-423e-84b4-9fdd59f28c7a","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Seq2Seq(\n","  (encoder): Encoder(\n","    (lstm): LSTM(40, 256, dropout=0.5, bidirectional=True)\n","  )\n","  (decoder): Decoder(\n","    (attention): Attn_(\n","      (attn): Linear(in_features=512, out_features=256, bias=True)\n","    )\n","    (fc): Linear(in_features=512, out_features=256, bias=True)\n","    (out): Linear(in_features=512, out_features=1, bias=True)\n","    (sig): Sigmoid()\n","  )\n",")\n"]}]},{"cell_type":"code","metadata":{"id":"7e779c1c-166a-44dc-8eaa-bcbd8ef44d6a","outputId":"5ef0457c-498c-4959-8c2a-801972a40c7d"},"source":["dataiter = iter(train_loader)\n","data = dataiter.next()\n","images, labels = data['data'],data['label']\n","print(images.shape)\n","print(labels.shape)"],"id":"7e779c1c-166a-44dc-8eaa-bcbd8ef44d6a","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([12, 40, 8064])\n","torch.Size([12, 1])\n"]}]},{"cell_type":"code","metadata":{"id":"ae528fbd-5e6c-441e-a438-b89cfb9c6a68","outputId":"97233e16-b0b2-4b26-c7ed-5dc526d5d425"},"source":["#Training the model\n","for epoch in range(15):\n","\n","    #model.train\n","    s2s.train()\n","    train_loss = 0\n","    \n","    ## training bathces in gpu\n","    for i, batch in enumerate(train_loader):\n","        data = batch['data'].permute(2, 0, 1).cuda()\n","        label = batch['label'].cuda()\n","        \n","        optimizer.zero_grad()\n","        output = s2s(data)\n","        loss = loss_fn(output, label)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        train_loss += loss.item()\n","\n","    ## evaluating the trained model on validation set\n","    s2s.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for i, batch in enumerate(val_loader):\n","\n","            data = batch['data'].permute(2, 0, 1).cuda()\n","            label = batch['label'].cuda()\n","            output = s2s(data)\n","            loss = loss_fn(output, label)\n","            val_loss += loss.item()\n","\n","    print('Epoch : {} train_loss : {} val_loss : {}'.format(epoch, (opt_weight*train_loss)/len(train_loader), (opt_weight*val_loss)/len(val_loader))) "],"id":"ae528fbd-5e6c-441e-a438-b89cfb9c6a68","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch : 0 train_loss : 0.30612760579295806 val_loss : 0.4290378889604049\n","Epoch : 1 train_loss : 0.4120200511133948 val_loss : 0.4290378889604049\n","Epoch : 2 train_loss : 0.4147645458842433 val_loss : 0.4290378889604049\n","Epoch : 3 train_loss : 0.4142776340218477 val_loss : 0.4290378889604049\n","Epoch : 4 train_loss : 0.41259594016851386 val_loss : 0.4290378889604049\n","Epoch : 5 train_loss : 0.4134486240120821 val_loss : 0.4290378889604049\n","Epoch : 6 train_loss : 0.4137219103436138 val_loss : 0.4290378889604049\n","Epoch : 7 train_loss : 0.41267346084949585 val_loss : 0.4290378889604049\n","Epoch : 8 train_loss : 0.4138711331389671 val_loss : 0.4290378889604049\n","Epoch : 9 train_loss : 0.4133401276344477 val_loss : 0.4290378889604049\n","Epoch : 10 train_loss : 0.4127664833955987 val_loss : 0.4290378889604049\n","Epoch : 11 train_loss : 0.41342927444812866 val_loss : 0.4290378889604049\n","Epoch : 12 train_loss : 0.41276066802268807 val_loss : 0.4290378889604049\n","Epoch : 13 train_loss : 0.412809117516806 val_loss : 0.4290378889604049\n","Epoch : 14 train_loss : 0.41334981554608013 val_loss : 0.4290378889604049\n"]}]},{"cell_type":"code","metadata":{"id":"0240c7d7-ef4e-4055-a3b8-e32f2c91398d","outputId":"437aaf37-132f-4564-b390-b7d087d5b1a1"},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","\n","#Calculating the metrics\n","fin_targets = []\n","fin_outputs = []\n","\n","with torch.no_grad():\n","    for i, batch in enumerate(train_loader):\n","\n","        data = batch['data'].permute(2, 0, 1).cuda()\n","        label = batch['label']\n","        output = s2s(data)\n","        fin_targets.append(np.asarray(label.numpy(),dtype=np.int))\n","        fin_outputs.append(np.asarray((output.cpu().detach().numpy()>0.5), dtype=np.int))\n","acc,precision,recall,f1score=classification_report(fin_outputs,fin_targets,best_class_weights)\n","print('Accuracy : {}'.format(acc))\n","print('Precision: {}'.format(precision))\n","print('Recall: {}'.format(recall))\n","print('F1score: {}'.format(f1score))"],"id":"0240c7d7-ef4e-4055-a3b8-e32f2c91398d","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy : 0.811\n","Precision: 0.889\n","Recall: 0.847\n","F1score: 0.8\n"]}]}]}