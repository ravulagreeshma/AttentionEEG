{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7adf96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b9f6ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeapS2SDatasetClassification(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "\n",
    "        _, _, filenames = next(os.walk(path))\n",
    "        filenames = sorted(filenames)\n",
    "        all_data = []\n",
    "        all_label = []\n",
    "        for dat in filenames:\n",
    "            temp = pickle.load(open(os.path.join(path, dat), 'rb'), encoding='latin1')\n",
    "            all_data.append(temp['data'][:, :32, :])\n",
    "            all_label.append(temp['labels'][:,:2])\n",
    "\n",
    "        self.data = np.vstack(all_data)\n",
    "        self.label = np.vstack(all_label)\n",
    "        del temp, all_data, all_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        single_data = self.data[idx]\n",
    "        single_label = (self.label[idx] > 5).astype(float)\n",
    "        \n",
    "        batch = {\n",
    "            'data': torch.Tensor(single_data),\n",
    "            'label': torch.Tensor(single_label)\n",
    "        }\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8d9822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960\n",
      "320\n"
     ]
    }
   ],
   "source": [
    "#@title Dataset Parameters { vertical-output: true }\n",
    "batch_size = 32 #@param {type:\"integer\"}\n",
    "\n",
    "dataset = DeapS2SDatasetClassification('data_preprocessed_python')\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_ind = int(0.75 * len(dataset))\n",
    "train_set = torch.utils.data.Subset(dataset, indices[:train_ind])\n",
    "val_set = torch.utils.data.Subset(dataset, indices[train_ind:])\n",
    "del dataset\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(val_set))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4c83ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationLSTM(nn.Module):\n",
    "    def __init__(self, in_features=32, emb_dim1=64, emb_dim2=32, out_features=2):\n",
    "        super(ClassificationLSTM, self).__init__()\n",
    "\n",
    "        self.emb_dim1 = emb_dim1\n",
    "        self.emb_dim2 = emb_dim2\n",
    "\n",
    "        self.lstm1 = nn.LSTM(in_features, emb_dim1, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(emb_dim1, emb_dim2, batch_first=True)\n",
    "        self.out = nn.Linear(emb_dim2, out_features)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0, c0 = self.init_hidden(x, self.emb_dim1)\n",
    "        out1, (h1, c1) = self.lstm1(x, (h0,c0))\n",
    "        out1 = self.drop(self.relu(out1))\n",
    "\n",
    "        h0, c0 = self.init_hidden(x, self.emb_dim2)\n",
    "        _, (h2, c2) = self.lstm2(out1, (h0,c0))\n",
    "        hidden = self.sig(h2.squeeze())\n",
    "        return self.sig(self.out(hidden))\n",
    "\n",
    "    def init_hidden(self, x, hidden_dim):\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_dim)\n",
    "        c0 = torch.zeros(1, x.size(0), hidden_dim)\n",
    "        return [t.cuda() for t in (h0, c0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd6c450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationLSTM()\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "EPOCH = 30\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6939ae09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 train_loss : 0.6858895162741343 val_loss : 0.6831320583820343\n",
      "Epoch : 1 train_loss : 0.675112928946813 val_loss : 0.6751194655895233\n",
      "Epoch : 2 train_loss : 0.6604529281457265 val_loss : 0.6766663432121277\n",
      "Epoch : 3 train_loss : 0.6465837200482686 val_loss : 0.6720701456069946\n",
      "Epoch : 4 train_loss : 0.6315569778283437 val_loss : 0.6795907080173492\n",
      "Epoch : 5 train_loss : 0.6218773603439331 val_loss : 0.6777112424373627\n",
      "Epoch : 6 train_loss : 0.5995577911535899 val_loss : 0.6812864303588867\n",
      "Epoch : 7 train_loss : 0.5851494034131368 val_loss : 0.6853281795978546\n",
      "Epoch : 8 train_loss : 0.5712579270203908 val_loss : 0.6986408233642578\n",
      "Epoch : 9 train_loss : 0.5511759559313456 val_loss : 0.7008939206600189\n",
      "Epoch : 10 train_loss : 0.5434943576653798 val_loss : 0.7075961589813232\n",
      "Epoch : 11 train_loss : 0.5267815818389256 val_loss : 0.7224518597126007\n",
      "Epoch : 12 train_loss : 0.5171008308728536 val_loss : 0.7251432538032532\n",
      "Epoch : 13 train_loss : 0.5102237741152446 val_loss : 0.7381891131401062\n",
      "Epoch : 14 train_loss : 0.51906232436498 val_loss : 0.7448485255241394\n",
      "Epoch : 15 train_loss : 0.48678512970606486 val_loss : 0.7544762074947358\n",
      "Epoch : 16 train_loss : 0.4789515405893326 val_loss : 0.7659168899059295\n",
      "Epoch : 17 train_loss : 0.4672282586495082 val_loss : 0.7796679198741913\n",
      "Epoch : 18 train_loss : 0.46449176371097567 val_loss : 0.7827184975147248\n",
      "Epoch : 19 train_loss : 0.4664510875940323 val_loss : 0.7949517071247101\n",
      "Epoch : 20 train_loss : 0.4513180414835612 val_loss : 0.806186991930008\n",
      "Epoch : 21 train_loss : 0.43977177441120147 val_loss : 0.7912384688854217\n",
      "Epoch : 22 train_loss : 0.4267045706510544 val_loss : 0.8147126376628876\n",
      "Epoch : 23 train_loss : 0.42156879405180614 val_loss : 0.8083283007144928\n",
      "Epoch : 24 train_loss : 0.424303729335467 val_loss : 0.8025632321834564\n",
      "Epoch : 25 train_loss : 0.4130864441394806 val_loss : 0.8360602974891662\n",
      "Epoch : 26 train_loss : 0.40670588811238606 val_loss : 0.8538649201393127\n",
      "Epoch : 27 train_loss : 0.40719439287980397 val_loss : 0.8419604122638702\n",
      "Epoch : 28 train_loss : 0.3929635415474574 val_loss : 0.8648765802383422\n",
      "Epoch : 29 train_loss : 0.39024281601111094 val_loss : 0.8400284051895142\n"
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "val_over_all = np.inf\n",
    "for epoch in range(EPOCH):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        data = batch['data'].permute(0,2,1).cuda()\n",
    "        label = batch['label'].cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss_list.append(train_loss/len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "\n",
    "            data = batch['data'].permute(0,2,1).cuda()\n",
    "            label = batch['label'].cuda()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, label)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss_list.append(val_loss/len(val_loader))\n",
    "    print('Epoch : {} train_loss : {} val_loss : {}'.format(epoch, train_loss/len(train_loader), val_loss/len(val_loader)))  \n",
    "\n",
    "    if val_loss_list[-1] < val_over_all:\n",
    "        val_over_all = val_loss_list[-1]\n",
    "        ckpt = {\n",
    "            'model_dict': model.state_dict(),\n",
    "            'eval_loss': val_loss_list[-1]\n",
    "        }\n",
    "\n",
    "        torch.save(ckpt, 'base_lstm.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19759cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-0041289ef37f>:19: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fin_outputs.append(np.asarray((output.cpu().detach().numpy()>0.5), dtype=np.int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.584375\n",
      "Precision: 0.8842975206611571\n",
      "Recall: 0.5889908256880734\n",
      "F1score: 0.7070484581497798\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "ckpt = torch.load('base_lstm.pt')\n",
    "model.load_state_dict(ckpt['model_dict'])\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# calculate Accuracy only\n",
    "\n",
    "fin_targets = []\n",
    "fin_outputs = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_loader):\n",
    "\n",
    "        data = batch['data'].permute(0,2,1).cuda()\n",
    "        label = batch['label']\n",
    "        output = model(data)\n",
    "        fin_targets.append(label.numpy())\n",
    "        fin_outputs.append(np.asarray((output.cpu().detach().numpy()>0.5), dtype=np.int))\n",
    "\n",
    "acc = accuracy_score(np.vstack(fin_outputs).flatten(), np.vstack(fin_targets).flatten())\n",
    "precision = precision_score(np.vstack(fin_outputs).flatten(), np.vstack(fin_targets).flatten())\n",
    "recall = recall_score(np.vstack(fin_outputs).flatten(), np.vstack(fin_targets).flatten())\n",
    "f1score = f1_score(np.vstack(fin_outputs).flatten(), np.vstack(fin_targets).flatten())\n",
    "\n",
    "print('Accuracy : {}'.format(acc))\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall: {}'.format(recall))\n",
    "print('F1score: {}'.format(f1score))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
