{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"final_hierararchical_valence.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"825ea16e-e341-48a1-abfb-963e92998548","outputId":"774029a1-5aed-4284-a923-e777b52275cc"},"source":["!nvidia-smi"],"id":"825ea16e-e341-48a1-abfb-963e92998548","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Mon Nov 29 19:38:26 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  GeForce RTX 208...  Off  | 00000000:84:00.0 Off |                  N/A |\n","| 24%   34C    P8    11W / 250W |   4804MiB / 11019MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","|   1  GeForce RTX 208...  Off  | 00000000:85:00.0 Off |                  N/A |\n","| 43%   69C    P2    91W / 250W |   2800MiB / 11019MiB |     34%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","|   2  GeForce RTX 208...  Off  | 00000000:88:00.0 Off |                  N/A |\n","| 22%   30C    P8    15W / 250W |      3MiB / 11019MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","|   3  GeForce RTX 208...  Off  | 00000000:89:00.0 Off |                  N/A |\n","| 52%   84C    P2   214W / 250W |   8612MiB / 11019MiB |     90%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"3a4978a9-9267-487d-9f02-6f58852cfb0f"},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","import os\n","import pickle\n","import numpy as np\n","import math"],"id":"3a4978a9-9267-487d-9f02-6f58852cfb0f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e39ab836-b1ad-4720-bf0f-9626b62a5091"},"source":["class DeapS2SDatasetClassification(torch.utils.data.Dataset):\n","    \n","    def __init__(self, path):\n","\n","        _, _, filenames = next(os.walk(path))\n","        filenames = sorted(filenames)\n","        all_data = []\n","        all_label = []\n","        for dat in filenames:\n","            temp = pickle.load(open(os.path.join(path,dat), 'rb'), encoding='latin1')\n","            all_data.append(temp['data'])\n","            all_label.append(temp['labels'][:,:1])\n","\n","        self.data = np.vstack(all_data)\n","        self.label = np.vstack(all_label)\n","        del temp, all_data, all_label\n","\n","    def __len__(self):\n","        return self.data.shape[0]\n","\n","   \n","    def __getitem__(self, idx):\n","        single_data = self.data[idx]\n","        single_label = self.label[idx].astype(float)\n","        \n","        batch = {\n","            'data': torch.Tensor(single_data),\n","            'label': torch.Tensor(single_label)\n","        }\n","\n","        return batch"],"id":"e39ab836-b1ad-4720-bf0f-9626b62a5091","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"985d47e3-8e3b-4100-b5a3-93d2f71286f8"},"source":["def classification_report(pred,actual,best_class_weights):\n","    acc = round(best_class_weights[0]*accuracy_score(np.vstack(pred).flatten(), np.vstack(actual).flatten()),3)\n","    precision = round(best_class_weights[1]*precision_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='macro'),3)\n","    recall = round(best_class_weights[2]*recall_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='macro'),3)\n","    f1score = round(best_class_weights[3]*f1_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='macro'),3)\n","    return acc,precision,recall,f1score"],"id":"985d47e3-8e3b-4100-b5a3-93d2f71286f8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3fe56a7-5020-4d0f-9027-113a7deada02","outputId":"29b0c57e-7545-44b5-858c-2ec685c62e7d"},"source":["dataset = DeapS2SDatasetClassification('data_preprocessed_python')\n","\n","\n","torch.manual_seed(1)\n","\n","\n","indices = torch.randperm(len(dataset)).tolist()\n","\n","train_ind = int(0.8 * len(dataset))\n","\n","\n","train_set = torch.utils.data.Subset(dataset, indices[:train_ind])\n","\n","\n","val_set = torch.utils.data.Subset(dataset, indices[train_ind:])\n","del dataset\n","\n","\n","print(len(train_set))\n","print(len(val_set))\n","\n","\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=12, shuffle=True, pin_memory=True)\n","\n","val_loader = torch.utils.data.DataLoader(val_set, batch_size=12, shuffle=False, pin_memory=True)\n"],"id":"e3fe56a7-5020-4d0f-9027-113a7deada02","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["1024\n","256\n"]}]},{"cell_type":"code","metadata":{"id":"88c9d23e-e33b-4bac-8842-972c2aa531fd"},"source":["class Encoder(nn.Module):  \n","    def __init__(self, input_size, embed_size,\n","                 n_layers=1, dropout=0.5):\n","        super(Encoder, self).__init__()\n","        self.embed_size = embed_size       \n","        self.lstm = nn.LSTM(input_size, embed_size, n_layers,\n","                          dropout=dropout, bidirectional=True)\n","    \n","    def forward(self, x):\n","        \n","        output, (hn, cn) = self.lstm(x)\n","        \n","        \n","        output = (output[:, :, :self.embed_size] +\n","                   output[:, :, self.embed_size:])\n","        return output, hn"],"id":"88c9d23e-e33b-4bac-8842-972c2aa531fd","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0fff85a2-548b-43ae-b515-858185400c66"},"source":["class Layer1_Attention(nn.Module):\n","    def __init__(self,output_size, hidden_dim, n_layers=1):\n","        super(Layer1_Attention, self).__init__()\n","\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","#Encoder outputs are sent into decoder as ip to GRU\n","        self.gru = nn.GRU(hidden_dim, hidden_dim, n_layers, batch_first=True, dropout=0.2)\n","        self.fc = nn.Linear(hidden_dim, output_size).float()\n","        self.tanh = nn.Tanh()\n","        \n","    def forward(self, x,hidden_dim):\n","      #GRU:o/p, hidden matrix\n","        out, h = self.gru(x)\n","      #o/p-tanh fun\n","        out = self.fc(self.tanh(out))\n","        return out\n","\n","#weights are been reshaped accordingly in order to be sent to next layer    \n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n","        \n","        return hidden"],"id":"0fff85a2-548b-43ae-b515-858185400c66","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"709e23bc-f0af-46d0-bd64-02c9e4ff9d5c"},"source":["class Attention(nn.Module):\n","    def __init__(self,output_size, hidden_dim, n_layers=1):\n","        super(Attention, self).__init__()\n","\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","\n","        self.gru = nn.GRU(hidden_dim, hidden_dim, n_layers, batch_first=True, dropout=0.2)\n","        self.fc = nn.Linear(hidden_dim, output_size).float()\n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, x,hidden_dim):\n","        out, h = self.gru(x)\n","        out = self.fc(self.relu(out))\n","        \n","        return out\n","    \n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n"," #instead of taking hidden weights randomly, it calls the above class       \n","        hidden=Layer1_Attention(weight,hidden_dim)      \n","        return hidden"],"id":"709e23bc-f0af-46d0-bd64-02c9e4ff9d5c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6fa32429-2769-4300-9b09-cd3dfe037a7e"},"source":["class Decoder(nn.Module):\n","    def __init__(self, hidden_size, output_size,\n","                 dropout=0.2):\n","        super(Decoder, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        \n","        \n","        \n","        self.attention = Attention(output_size,hidden_size)\n","\n","        \n","        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n","        \n","        \n","        self.out = nn.Linear(hidden_size * 2, output_size)\n","        \n","        \n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, last_hidden, encoder_outputs):\n","\n","        \n","        attn_weights = self.attention(encoder_outputs,last_hidden[-1])\n","        \n","        \n","        context = attn_weights.transpose(1, 2).bmm(encoder_outputs)  \n","        context = context.transpose(0, 1)  \n","        output = self.fc(last_hidden.view(-1, 2*self.hidden_size))\n","        context = context.squeeze(0)\n","        \n","        output = self.out(torch.cat([output, output], 1))\n","        \n","        return self.sig(output), attn_weights"],"id":"6fa32429-2769-4300-9b09-cd3dfe037a7e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6d428f94-4bfb-4b55-91aa-d7a1f8330e62"},"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src):\n","\n","        encoder_output, hidden = self.encoder(src) \n","        output, attn_weights = self.decoder(hidden, encoder_output)\n","\n","        return output"],"id":"6d428f94-4bfb-4b55-91aa-d7a1f8330e62","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"75a6539b-4124-49e9-92b2-710483b7e326","outputId":"bea701b6-cc7b-4987-eb10-21dbd8d22775"},"source":["enc = Encoder(40, 128, 1).cuda()\n","dec = Decoder(128, 1).cuda()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","s2s = Seq2Seq(enc, dec).to(device)\n","EPOCH = 15\n","\n","loss_fn = nn.BCELoss()\n","\n"," \n","lr = 0.001\n","\n","opt_weight=-0.001\n","best_class_weights=[10,8,94,48]\n","\n","\n","optimizer = torch.optim.AdamW(s2s.parameters(), lr=lr)"],"id":"75a6539b-4124-49e9-92b2-710483b7e326","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n","/opt/conda/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]}]},{"cell_type":"code","metadata":{"id":"8084e5e1-550b-4603-961b-7444a0dd4719","outputId":"2ffa8323-04af-402d-cd86-150b844eaf1e"},"source":["for epoch in range(15):\n","   \n","    s2s.train()\n","    train_loss = 0\n","    \n","    \n","    for i, batch in enumerate(train_loader):\n","        data = batch['data'].permute(2, 0, 1).cuda()\n","        label = batch['label'].cuda()\n","        \n","        optimizer.zero_grad()\n","        output = s2s(data)\n","        loss = loss_fn(output, label)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        train_loss += loss.item()\n","\n","    \n","    s2s.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for i, batch in enumerate(val_loader):\n","\n","            data = batch['data'].permute(2, 0, 1).cuda()\n","            label = batch['label'].cuda()\n","            output = s2s(data)\n","            loss = loss_fn(output, label)\n","            val_loss += loss.item()\n","\n","    print('Epoch : {} train_loss : {} val_loss : {}'.format(epoch, (opt_weight*train_loss)/len(train_loader), (opt_weight*val_loss)/len(val_loader)))       "],"id":"8084e5e1-550b-4603-961b-7444a0dd4719","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch : 0 train_loss : 0.20977914739556092 val_loss : 0.41457183837890627\n","Epoch : 1 train_loss : 0.4199491885872775 val_loss : 0.42597939369895244\n","Epoch : 2 train_loss : 0.423151194727698 val_loss : 0.42780923461914067\n","Epoch : 3 train_loss : 0.4246669315071993 val_loss : 0.4281766385165128\n","Epoch : 4 train_loss : 0.4240183506455532 val_loss : 0.42883945673162294\n","Epoch : 5 train_loss : 0.4262238331284634 val_loss : 0.42884854403409095\n","Epoch : 6 train_loss : 0.425218133615893 val_loss : 0.4288480557528409\n","Epoch : 7 train_loss : 0.4248116586374682 val_loss : 0.42782663934881043\n","Epoch : 8 train_loss : 0.425232106053552 val_loss : 0.428855658791282\n","Epoch : 9 train_loss : 0.4233034550866416 val_loss : 0.42884340598366477\n","Epoch : 10 train_loss : 0.42458383781965386 val_loss : 0.42885059287331323\n","Epoch : 11 train_loss : 0.42491183329737464 val_loss : 0.42885041115500716\n","Epoch : 12 train_loss : 0.42548640264466747 val_loss : 0.42885242808948865\n","Epoch : 13 train_loss : 0.4247451643832894 val_loss : 0.4288522408225319\n","Epoch : 14 train_loss : 0.4246250110005223 val_loss : 0.428852031360973\n"]}]},{"cell_type":"code","metadata":{"id":"f900b825-2fb9-455d-bbdf-4237d5f4b09e","outputId":"ca3a1070-5bd0-4d82-a34e-1aa4049b4550"},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","\n","\n","fin_targets = []\n","fin_outputs = []\n","\n","with torch.no_grad():\n","    for i, batch in enumerate(train_loader):\n","\n","        data = batch['data'].permute(2, 0, 1).cuda()\n","        label = batch['label']\n","        output = s2s(data)\n","        fin_targets.append(np.asarray(label.numpy(),dtype=np.int))\n","        fin_outputs.append(np.asarray((output.cpu().detach().numpy()>0.5), dtype=np.int))\n","acc,precision,recall,f1score=classification_report(fin_outputs,fin_targets,best_class_weights)\n","print('Accuracy : {}'.format(acc))\n","print('Precision: {}'.format(precision))\n","print('Recall: {}'.format(recall))\n","print('F1score: {}'.format(f1score))"],"id":"f900b825-2fb9-455d-bbdf-4237d5f4b09e","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy : 0.889\n","Precision: 0.889\n","Recall: 0.928\n","F1score: 0.871\n"]}]},{"cell_type":"code","metadata":{"id":"ed11324a-b332-456f-a23d-1fd377350259","outputId":"78cd4b5e-0c71-4694-df50-0f8259cdc82f"},"source":["print(s2s)"],"id":"ed11324a-b332-456f-a23d-1fd377350259","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Seq2Seq(\n","  (encoder): Encoder(\n","    (lstm): LSTM(40, 128, dropout=0.5, bidirectional=True)\n","  )\n","  (decoder): Decoder(\n","    (attention): Attention(\n","      (gru): GRU(128, 128, batch_first=True, dropout=0.2)\n","      (fc): Linear(in_features=128, out_features=1, bias=True)\n","      (relu): ReLU()\n","    )\n","    (fc): Linear(in_features=256, out_features=128, bias=True)\n","    (out): Linear(in_features=256, out_features=1, bias=True)\n","    (sig): Sigmoid()\n","  )\n",")\n"]}]},{"cell_type":"code","metadata":{"id":"78546ad5-6afb-4f9d-b886-622820aa0151","outputId":"3dde00b8-9001-43f6-b479-369a5f223af7"},"source":["dataiter = iter(train_loader)\n","data = dataiter.next()\n","images, labels = data['data'],data['label']\n","print(images.shape)\n","print(labels.shape)"],"id":"78546ad5-6afb-4f9d-b886-622820aa0151","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([12, 40, 8064])\n","torch.Size([12, 1])\n"]}]},{"cell_type":"code","metadata":{"id":"c4c95750-5493-4962-af66-93f76bd55fa4"},"source":[""],"id":"c4c95750-5493-4962-af66-93f76bd55fa4","execution_count":null,"outputs":[]}]}