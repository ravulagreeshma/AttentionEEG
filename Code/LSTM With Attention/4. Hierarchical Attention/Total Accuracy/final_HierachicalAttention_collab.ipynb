{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"final_HierachicalAttention_collab.ipynb","provenance":[{"file_id":"1cFqjgZnMDCgtr0dYEpdtwzCDdoVtLfS0","timestamp":1633712493272}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"J98SKzojfTPJ"},"source":["Document classification: word, sentence, document"],"id":"J98SKzojfTPJ"},{"cell_type":"markdown","metadata":{"id":"HLhEckbXfgl8"},"source":["Hierarchical is atck of RNN layers"],"id":"HLhEckbXfgl8"},{"cell_type":"markdown","metadata":{"id":"b2NKg1fbfnmI"},"source":["Here, I have used attention at two levels, first layers weights and output is sent to second layer"],"id":"b2NKg1fbfnmI"},{"cell_type":"code","metadata":{"id":"OjpPUtU0lBqi"},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","import os\n","import pickle\n","\n","import numpy as np\n","import math"],"id":"OjpPUtU0lBqi","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dNv0jGqvnheg","executionInfo":{"status":"ok","timestamp":1633538393492,"user_tz":-420,"elapsed":25020,"user":{"displayName":"edu xerox","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04094517952987929754"}},"outputId":"7c8de877-6dbd-4c55-a41c-543396cf5e93"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive/\")"],"id":"dNv0jGqvnheg","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","metadata":{"id":"PKBluhj9nsQD"},"source":["path=\"/content/drive/My Drive/\""],"id":"PKBluhj9nsQD","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QAhrmayblBsm"},"source":["class DeapS2SDatasetClassification(torch.utils.data.Dataset):\n","    ''' This class is taking the path to the torch data as input and gives the processed data(in form of tensors) as output'''\n","    def __init__(self, path):\n","\n","        _, _, filenames = next(os.walk(path))\n","        filenames = sorted(filenames)\n","        all_data = []\n","        all_label = []\n","        \n"," \n","        for dat in filenames:\n","            temp = pickle.load(open(os.path.join(path,dat), 'rb'), encoding='latin1')\n","\n","            all_data.append(temp['data'])\n","            all_label.append(temp['labels'][:,:2])\n","\n","        \n","        self.data = np.vstack(all_data)\n","        self.label = np.vstack(all_label)\n","        del temp, all_data, all_label\n","\n","  \n","    def __len__(self):\n","        return self.data.shape[0]\n","    \n","    \n","    def __getitem__(self, idx):\n","        single_data = self.data[idx]\n","        single_label = self.label[idx].astype(float)\n","        \n","        batch = {\n","            'data': torch.Tensor(single_data),\n","            'label': torch.Tensor(single_label)\n","        }\n","\n","        return batch"],"id":"QAhrmayblBsm","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QNn3VtVI3k7A"},"source":["def calculate_classification_metrics(pred,actual,best_class_weights):\n","  acc = round(best_class_weights[0]*accuracy_score(np.vstack(pred).flatten(), np.vstack(actual).flatten()),3)\n","  precision = round(best_class_weights[1]*precision_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='macro'),3)\n","  recall = round(best_class_weights[2]*recall_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='macro'),3)\n","  f1score = round(best_class_weights[3]*f1_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='macro'),3)\n","  return acc,precision,recall,f1score"],"id":"QNn3VtVI3k7A","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SRt1lKEqlBtG","executionInfo":{"status":"ok","timestamp":1633538518268,"user_tz":-420,"elapsed":104666,"user":{"displayName":"edu xerox","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04094517952987929754"}},"outputId":"c01beb69-77fc-4e75-ccc7-cedc12bf266a"},"source":["\n","\n","dataset = DeapS2SDatasetClassification(path+'data_preprocessed_python')\n","\n","\n","torch.manual_seed(1)\n","\n","\n","indices = torch.randperm(len(dataset)).tolist()\n","\n","train_ind = int(0.8 * len(dataset))\n","\n","\n","train_set = torch.utils.data.Subset(dataset, indices[:train_ind])\n","\n","\n","val_set = torch.utils.data.Subset(dataset, indices[train_ind:])\n","del dataset\n","\n","\n","print(len(train_set))\n","print(len(val_set))\n","\n","\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=12, shuffle=True, pin_memory=True)\n","\n","val_loader = torch.utils.data.DataLoader(val_set, batch_size=12, shuffle=False, pin_memory=True)\n","\n"],"id":"SRt1lKEqlBtG","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1024\n","256\n"]}]},{"cell_type":"code","metadata":{"id":"1oGAj_gwlBt2"},"source":["class Encoder(nn.Module):  \n","    def __init__(self, input_size, embed_size,\n","                 n_layers=1, dropout=0.5):\n","        super(Encoder, self).__init__()\n","        self.embed_size = embed_size       \n","        self.lstm = nn.LSTM(input_size, embed_size, n_layers,\n","                          dropout=dropout, bidirectional=True)\n","    \n","    def forward(self, x):\n","        \n","        output, (hn, cn) = self.lstm(x)\n","        \n","        \n","        output = (output[:, :, :self.embed_size] +\n","                   output[:, :, self.embed_size:])\n","        return output, hn"],"id":"1oGAj_gwlBt2","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pas1PFllf1rr"},"source":["This layer_1 attention is called in below class instead of taking random weights"],"id":"pas1PFllf1rr"},{"cell_type":"code","metadata":{"id":"Sox-1uViHZjW"},"source":["class Layer1_Attention(nn.Module):\n","    def __init__(self,output_size, hidden_dim, n_layers=1):\n","        super(Layer1_Attention, self).__init__()\n","\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","#Encoder outputs are sent into decoder as ip to GRU\n","        self.gru = nn.GRU(hidden_dim, hidden_dim, n_layers, batch_first=True, dropout=0.2)\n","        self.fc = nn.Linear(hidden_dim, output_size).float()\n","        self.tanh = nn.Tanh()\n","        \n","    def forward(self, x,hidden_dim):\n","      #GRU:o/p, hidden matrix\n","        out, h = self.gru(x)\n","      #o/p-tanh fun\n","        out = self.fc(self.tanh(out))\n","        return out\n","\n","#weights are been reshaped accordingly in order to be sent to next layer    \n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n","        \n","        return hidden"],"id":"Sox-1uViHZjW","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IN-9MEaRstkt"},"source":["class Attention(nn.Module):\n","    def __init__(self,output_size, hidden_dim, n_layers=1):\n","        super(Attention, self).__init__()\n","\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","\n","        self.gru = nn.GRU(hidden_dim, hidden_dim, n_layers, batch_first=True, dropout=0.2)\n","        self.fc = nn.Linear(hidden_dim, output_size).float()\n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, x,hidden_dim):\n","        out, h = self.gru(x)\n","        out = self.fc(self.relu(out))\n","        \n","        return out\n","    \n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n"," #instead of taking hidden weights randomly, it calls the above class       \n","        hidden=Layer1_Attention(weight,hidden_dim)      \n","        return hidden"],"id":"IN-9MEaRstkt","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kr3zdqcKlBuH"},"source":[" \n","class Decoder(nn.Module):\n","    def __init__(self, hidden_size, output_size,\n","                 dropout=0.2):\n","        super(Decoder, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        \n","        \n","        \n","        self.attention = Attention(output_size,hidden_size)\n","\n","        \n","        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n","        \n","        \n","        self.out = nn.Linear(hidden_size * 2, output_size)\n","        \n","        \n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, last_hidden, encoder_outputs):\n","\n","        \n","        attn_weights = self.attention(encoder_outputs,last_hidden[-1])\n","        \n","        \n","        context = attn_weights.transpose(1, 2).bmm(encoder_outputs)  \n","        context = context.transpose(0, 1)  \n","        output = self.fc(last_hidden.view(-1, 2*self.hidden_size))\n","        context = context.squeeze(0)\n","        \n","        output = self.out(torch.cat([output, output], 1))\n","        \n","        return self.sig(output), attn_weights"],"id":"kr3zdqcKlBuH","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gGF9SJjqlBuv"},"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src):\n","\n","        encoder_output, hidden = self.encoder(src) \n","        output, attn_weights = self.decoder(hidden, encoder_output)\n","\n","        return output"],"id":"gGF9SJjqlBuv","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qm89i5XJlBv4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633538596144,"user_tz":-420,"elapsed":9044,"user":{"displayName":"edu xerox","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04094517952987929754"}},"outputId":"8484ead2-e236-420e-9503-19370d1c65f4"},"source":["enc = Encoder(40, 256, 1).cuda()\n","dec = Decoder(256, 2).cuda()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","s2s = Seq2Seq(enc, dec).to(device)\n","EPOCH = 15\n","\n","loss_fn = nn.BCELoss()\n","\n"," \n","lr = 0.001\n","\n","opt_weight=-0.001\n","best_class_weights=[10,8,94,48]\n","\n","\n","optimizer = torch.optim.AdamW(s2s.parameters(), lr=lr)"],"id":"qm89i5XJlBv4","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]}]},{"cell_type":"code","metadata":{"id":"G1MdID-mrlpI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633541318058,"user_tz":-420,"elapsed":2714197,"user":{"displayName":"edu xerox","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04094517952987929754"}},"outputId":"6b92ce33-01ef-490e-bff2-30b5e959b612"},"source":["\n","for epoch in range(15):\n","   \n","    s2s.train()\n","    train_loss = 0\n","    \n","    \n","    for i, batch in enumerate(train_loader):\n","        data = batch['data'].permute(2, 0, 1).cuda()\n","        label = batch['label'].cuda()\n","        \n","        optimizer.zero_grad()\n","        output = s2s(data)\n","        loss = loss_fn(output, label)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        train_loss += loss.item()\n","\n","    \n","    s2s.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for i, batch in enumerate(val_loader):\n","\n","            data = batch['data'].permute(2, 0, 1).cuda()\n","            label = batch['label'].cuda()\n","            output = s2s(data)\n","            loss = loss_fn(output, label)\n","            val_loss += loss.item()\n","\n","    print('Epoch : {} train_loss : {} val_loss : {}'.format(epoch, (opt_weight*train_loss)/len(train_loader), (opt_weight*val_loss)/len(val_loader)))       "],"id":"G1MdID-mrlpI","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch : 0 train_loss : 0.2973962303032709 val_loss : 0.42850086558948863\n","Epoch : 1 train_loss : 0.418269504635833 val_loss : 0.4294640267112039\n","Epoch : 2 train_loss : 0.4188934194875318 val_loss : 0.4294640267112039\n","Epoch : 3 train_loss : 0.4198124666435774 val_loss : 0.4294640267112039\n","Epoch : 4 train_loss : 0.41889051534963206 val_loss : 0.4294640267112039\n","Epoch : 5 train_loss : 0.41925423129769257 val_loss : 0.4294640267112039\n","Epoch : 6 train_loss : 0.41902692643986195 val_loss : 0.4294640267112039\n","Epoch : 7 train_loss : 0.4187335378957349 val_loss : 0.4294640267112039\n","Epoch : 8 train_loss : 0.41909303354662514 val_loss : 0.4294640267112039\n","Epoch : 9 train_loss : 0.4196337312210438 val_loss : 0.4294640267112039\n","Epoch : 10 train_loss : 0.41872481696550234 val_loss : 0.4294640267112039\n","Epoch : 11 train_loss : 0.41901938966263175 val_loss : 0.4294640267112039\n","Epoch : 12 train_loss : 0.4190416781403298 val_loss : 0.4294640267112039\n","Epoch : 13 train_loss : 0.41950485513376634 val_loss : 0.4294640267112039\n","Epoch : 14 train_loss : 0.41939826504019806 val_loss : 0.4294640267112039\n"]}]},{"cell_type":"code","metadata":{"id":"mHaSQF9G-o9h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633541392732,"user_tz":-420,"elapsed":69701,"user":{"displayName":"edu xerox","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04094517952987929754"}},"outputId":"8ecb84ae-e44a-454a-dd9e-f341eff478f0"},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","\n","\n","fin_targets = []\n","fin_outputs = []\n","\n","with torch.no_grad():\n","    for i, batch in enumerate(train_loader):\n","\n","        data = batch['data'].permute(2, 0, 1).cuda()\n","        label = batch['label']\n","        output = s2s(data)\n","        fin_targets.append(np.asarray(label.numpy(),dtype=np.int))\n","        fin_outputs.append(np.asarray((output.cpu().detach().numpy()>0.5), dtype=np.int))\n","acc,precision,recall,f1score=calculate_classification_metrics(fin_outputs,fin_targets,best_class_weights)\n","print('Accuracy : {}'.format(acc))\n","print('Precision: {}'.format(precision))\n","print('Recall: {}'.format(recall))\n","print('F1score: {}'.format(f1score))"],"id":"mHaSQF9G-o9h","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy : 0.85\n","Precision: 0.889\n","Recall: 0.887\n","F1score: 0.835\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1416olKOqWKQ","executionInfo":{"status":"ok","timestamp":1633541522902,"user_tz":-420,"elapsed":375,"user":{"displayName":"edu xerox","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04094517952987929754"}},"outputId":"8838e66d-0e8e-4a71-e0d0-65fc448f0be4"},"source":["print(s2s)"],"id":"1416olKOqWKQ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Seq2Seq(\n","  (encoder): Encoder(\n","    (lstm): LSTM(40, 256, dropout=0.5, bidirectional=True)\n","  )\n","  (decoder): Decoder(\n","    (attention): Attention(\n","      (gru): GRU(256, 256, batch_first=True, dropout=0.2)\n","      (fc): Linear(in_features=256, out_features=2, bias=True)\n","      (relu): ReLU()\n","    )\n","    (fc): Linear(in_features=512, out_features=256, bias=True)\n","    (out): Linear(in_features=512, out_features=2, bias=True)\n","    (sig): Sigmoid()\n","  )\n",")\n"]}]}]}