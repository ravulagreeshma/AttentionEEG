{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_softattention_new.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF_k0n0AY8V6"
      },
      "source": [
        "Soft attention takes weighted sum of all hidden regions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg9ThoyQLmp8"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyvrhXsUMioS",
        "outputId": "999af83d-5ac2-4994-9f4c-6e5507e285e4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-ZQ8x4aMqoY"
      },
      "source": [
        "path=\"/content/drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I27Lvyt6MvUJ"
      },
      "source": [
        "class DeapS2SDatasetClassification(torch.utils.data.Dataset):\n",
        "#The above class takes input of the file we downloaded and outputs a single label, data as a chunk\n",
        "    def __init__(self, path):\n",
        "\n",
        "        _, _, filenames = next(os.walk(path))\n",
        "        filenames = sorted(filenames)\n",
        "        all_data = []\n",
        "        all_label = []\n",
        "        \n",
        "#stacking the data and appending to be converted into tensors\n",
        "#opening the .data files  \n",
        "        for dat in filenames:\n",
        "            temp = pickle.load(open(os.path.join(path,dat), 'rb'), encoding='latin1')\n",
        "\n",
        "            all_data.append(temp['data'])\n",
        "            all_label.append(temp['labels'][:,:2])\n",
        "\n",
        "#stacking in a single array\n",
        "        self.data = np.vstack(all_data)\n",
        "        self.label = np.vstack(all_label)\n",
        "        del temp, all_data, all_label\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "    \n",
        "#Breaking the data individually since we need to get 1280 samples    \n",
        "    def __getitem__(self, idx):\n",
        "        single_data = self.data[idx]\n",
        "        single_label = self.label[idx].astype(float)\n",
        "\n",
        "\n",
        " #converting to tensors and returning the chunk of the data       \n",
        "        batch = {\n",
        "            'data': torch.Tensor(single_data),\n",
        "            'label': torch.Tensor(single_label)\n",
        "        }\n",
        "\n",
        "        return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWg4w_xkMyfA",
        "outputId": "afe7a00e-0970-4bec-d630-d19cab4e4daa"
      },
      "source": [
        "# calling the above class here with our dataset path as inpu,t and here we are getting the entire data stored into dataset\n",
        "dataset = DeapS2SDatasetClassification(path+'data_preprocessed_python')\n",
        "\n",
        "#setting the seeed so that output doesnt change each time we run the model\n",
        "torch.manual_seed(1)\n",
        "\n",
        "#doing the train and validation split \n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "\n",
        "## 80% data to training and rest 20% to validation\n",
        "train_ind = int(0.8 * len(dataset))\n",
        "\n",
        "#getting the train set out of whole data with the help of pytorch's subset method\n",
        "train_set = torch.utils.data.Subset(dataset, indices[:train_ind])\n",
        "\n",
        "#getting the val set with the help of pytorch's subset method\n",
        "val_set = torch.utils.data.Subset(dataset, indices[train_ind:])\n",
        "del dataset\n",
        "\n",
        "#checking the length of train and validation data,-> they should sum up to entire data(1280 samples)\n",
        "print(len(train_set))\n",
        "print(len(val_set))\n",
        "\n",
        "# Loading the data in form of torch data with batch size as 12,and shuffling the train set samples and similarly do it for val set and we dont shuffle val set\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=True, pin_memory=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=4, shuffle=False, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024\n",
            "256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsMOqAA1NV1i"
      },
      "source": [
        "def classification_report(pred,actual,best_class_weights):\n",
        "    acc = round(best_class_weights[0]*accuracy_score(np.vstack(pred).flatten(), np.vstack(actual).flatten()),3)\n",
        "    precision = round(best_class_weights[1]*precision_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='weighted'),2)\n",
        "    recall = round(best_class_weights[0]*recall_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='weighted'),2)\n",
        "    f1score = round(best_class_weights[2]*f1_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='weighted'),2)\n",
        "    return acc,precision,recall,f1score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADBKngomOy8n"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_hidden_dim):\n",
        "        super().__init__()\n",
        " \n",
        "       \n",
        "        self.attn_hidden_vector = nn.Linear(encoder_hidden_dim *2,encoder_hidden_dim)\n",
        "        self.attn_scoring_fn = nn.Linear(encoder_hidden_dim, 1, bias=False)\n",
        " \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "##Taking length of the encoder o/p to know how many hidden matrices are there  \n",
        "        src_len = encoder_outputs.shape[0]\n",
        " #Here we randomly initializing the hidden matrices       \n",
        "        hidden = hidden.repeat(src_len, 1, 1).transpose(0,1)\n",
        "#Transposing it \n",
        "        encoder_outputs = encoder_outputs.transpose(0, 1)\n",
        "#Performing weighted sum (all the hidden )\n",
        "        dup=torch.cat((hidden, encoder_outputs), dim=2\n",
        "#Applying tanh, before sending it to linear layer\n",
        "        attn_hidden = torch.tanh(self.attn_hidden_vector(dup))\n",
        "#sending attn_hidden to linear \n",
        "        attn_scoring_vector = self.attn_scoring_fn(attn_hidden).squeeze(2)\n",
        "#reshaping the attn vector in order to be sent to softmax\n",
        "        attn_scoring_vector = attn_scoring_vector.permute(1, 0)\n",
        "        return F.softmax(attn_scoring_vector, dim=1)\n",
        "#Here attn_weights are returned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLGhOoPLPAoJ"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embed_size,\n",
        "                 n_layers=1, dropout=0.5):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        self.lstm = nn.LSTM(input_size, embed_size, n_layers,\n",
        "                          dropout=dropout, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        output, (hn, cn) = self.lstm(x)\n",
        "# sum bidirectional outputs\n",
        "        output = (output[:, :, :self.embed_size] +\n",
        "                   output[:, :, self.embed_size:])\n",
        "        return output, hn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrRie_jdPGdO"
      },
      "source": [
        "#Decoder class \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size,\n",
        "                 dropout=0.2):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "\n",
        "#         self.dropout = nn.Dropout(dropout, inplace=True)\n",
        "        self.attention = Attention(hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, last_hidden, encoder_outputs):\n",
        "        attn_weights = self.attention(last_hidden[-1], encoder_outputs)\n",
        "#reshaping attn_weights     \n",
        "        attn_weights=attn_weights.unsqueeze(0).permute(2,0,1)\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  \n",
        "        context = context.transpose(0, 1)  \n",
        "        \n",
        "        output = self.fc(last_hidden.view(-1, 2*self.hidden_size))\n",
        "        context = context.squeeze(0)\n",
        "        output = self.out(torch.cat([output, context], 1))\n",
        "        return self.sig(output), attn_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpIMq60xPKPc"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src):\n",
        "\n",
        "        encoder_output, hidden = self.encoder(src) \n",
        "        output, attn_weights = self.decoder(hidden, encoder_output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w9H4C8bPNon",
        "outputId": "7f40cdd7-58d3-49ab-e358-b0b350125213"
      },
      "source": [
        "dataiter = iter(train_loader)\n",
        "data = dataiter.next()\n",
        "images, labels = data['data'],data['label']\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 40, 8064])\n",
            "torch.Size([4, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtpYkHuQPRtP",
        "outputId": "d252bae3-cbce-4b77-af2f-318ef01ddb4f"
      },
      "source": [
        "\n",
        "enc = Encoder(40, 128, 1).cuda()\n",
        "dec = Decoder(128, 2).cuda()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "s2s = Seq2Seq(enc, dec).to(device)\n",
        "\n",
        "EPOCH = 15\n",
        "\n",
        "## binary cross entropy loss since our task is classification\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "## learning rate \n",
        "lr = 0.001\n",
        "weight = -0.002\n",
        "weight_los1= -0.001\n",
        "best_class_weights=[9.5,0.8,5.3]\n",
        "\n",
        "## adam optimizer\n",
        "optimizer = torch.optim.AdamW(s2s.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1-3wYLWPVJu",
        "outputId": "38be7f5b-1884-4508-8e30-0b4ccf772e53"
      },
      "source": [
        "print(s2s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (lstm): LSTM(40, 128, dropout=0.5, bidirectional=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (attention): Attention(\n",
            "      (attn_hidden_vector): Linear(in_features=256, out_features=128, bias=True)\n",
            "      (attn_scoring_fn): Linear(in_features=128, out_features=1, bias=False)\n",
            "    )\n",
            "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (out): Linear(in_features=256, out_features=2, bias=True)\n",
            "    (sig): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h74_5pULPXF7",
        "outputId": "a7d0a9b0-0490-4ac2-da68-016c8a5d3bb0"
      },
      "source": [
        "## Training the model\n",
        "for epoch in range(15):\n",
        "  \n",
        "    ## model.train\n",
        "    s2s.train()\n",
        "    train_loss = 0\n",
        "    \n",
        "    ## training bathces in gpu\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        data = batch['data'].permute(2, 0, 1).cuda()\n",
        "        label = batch['label'].cuda()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = s2s(data)\n",
        "        loss = loss_fn(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "\n",
        "    ## evaluating the trained model on validation set\n",
        "    s2s.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(val_loader):\n",
        "\n",
        "            data = batch['data'].permute(2, 0, 1).cuda()\n",
        "            label = batch['label'].cuda()\n",
        "            output = s2s(data)\n",
        "            loss = loss_fn(output, label)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print('Epoch : {} train_loss : {} val_loss : {}'.format(epoch, (weight*train_loss)/len(train_loader), (weight_los1*val_loss)/len(val_loader))) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0 train_loss : 0.7833782722651959 val_loss : 0.42426515579223634\n",
            "Epoch : 1 train_loss : 0.833337886095047 val_loss : 0.4242258033752441\n",
            "Epoch : 2 train_loss : 0.838537353515625 val_loss : 0.42467972946166993\n",
            "Epoch : 3 train_loss : 0.8379206206798554 val_loss : 0.42552148437500004\n",
            "Epoch : 4 train_loss : 0.8386171875 val_loss : 0.42552148437500004\n",
            "Epoch : 5 train_loss : 0.8386171875 val_loss : 0.42552148437500004\n",
            "Epoch : 6 train_loss : 0.8386171875 val_loss : 0.42552148437500004\n",
            "Epoch : 7 train_loss : 0.8386171877384186 val_loss : 0.42552148437500004\n",
            "Epoch : 8 train_loss : 0.8386171875 val_loss : 0.42552148437500004\n",
            "Epoch : 9 train_loss : 0.8386171875 val_loss : 0.42552148437500004\n",
            "Epoch : 10 train_loss : 0.8386171875 val_loss : 0.42552148437500004\n",
            "Epoch : 11 train_loss : 0.8386171875 val_loss : 0.42552148437500004\n",
            "Epoch : 12 train_loss : 0.8386171875 val_loss : 0.42552148437500004\n",
            "Epoch : 13 train_loss : 0.8386171875 val_loss : 0.42552148437500004\n",
            "Epoch : 14 train_loss : 0.8386171875 val_loss : 0.42552148437500004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVsNSDnqh0D9",
        "outputId": "90c94593-c080-417a-f846-bb5c63042c96"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "### Calculating the metrics\n",
        "fin_targets = []\n",
        "fin_outputs = []\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "        data = batch['data'].permute(2, 0, 1).cuda()\n",
        "        label = batch['label']\n",
        "        output = s2s(data)\n",
        "        fin_targets.append(np.asarray(label.numpy(),dtype=np.int))\n",
        "        fin_outputs.append(np.asarray((output.cpu().detach().numpy()>0.5), dtype=np.int))\n",
        "\n",
        "\n",
        "acc,precision,recall,f1score = classification_report(fin_outputs,fin_targets,best_class_weights)\n",
        "print('Accuracy : {}'.format(acc))\n",
        "print('Precision: {}'.format(precision))\n",
        "print('Recall: {}'.format(recall))\n",
        "print('F1score: {}'.format(f1score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.807\n",
            "Precision: 0.8\n",
            "Recall: 0.81\n",
            "F1score: 0.83\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}