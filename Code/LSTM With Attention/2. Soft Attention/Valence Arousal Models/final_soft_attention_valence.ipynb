{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"final_soft_attention_valence.ipynb","provenance":[{"file_id":"1J3gCjH_VuIaMaZJWRtalPBWunAaZtSnx","timestamp":1638210433395},{"file_id":"1KxPoA_IHJqM_mesKaUFY4HXCeLJNS2lw","timestamp":1638155129134}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"cells":[{"cell_type":"code","metadata":{"id":"jg9ThoyQLmp8"},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","import os\n","import pickle\n","import numpy as np\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jyvrhXsUMioS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-ZQ8x4aMqoY"},"source":["path=\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I27Lvyt6MvUJ"},"source":["class DeapS2SDatasetClassification(torch.utils.data.Dataset):\n","    ''' This class is taking the path to the torch data as input and gives the processed data(in form of tensors) as output'''\n","    def __init__(self, path):\n","\n","        _, _, filenames = next(os.walk(path))\n","        filenames = sorted(filenames)\n","        all_data = []\n","        all_label = []\n","        \n","        ### opening the .datfiles and reading them and appending data and labels seperately\n","        for dat in filenames:\n","            temp = pickle.load(open(os.path.join(path,dat), 'rb'), encoding='latin1')\n","\n","            all_data.append(temp['data'])\n","            all_label.append(temp['labels'][:,:1])\n","#             break\n","        ### stacking the data so that this should be further converted to tensors to feed into models.\n","        self.data = np.vstack(all_data)\n","        self.label = np.vstack(all_label)\n","        del temp, all_data, all_label\n","\n","    ## just getting the length of the data\n","    def __len__(self):\n","        return self.data.shape[0]\n","    \n","     #### Till this point we have each .dat file's data stacked,\n","    # now with in each .dat file we have 40 samples,so we are seperating out each into single data and sinle label \n","    ## and convering them to tensor and returning the final data\n","    def __getitem__(self, idx):\n","        single_data = self.data[idx]\n","        single_label = self.label[idx].astype(float)\n","        \n","        batch = {\n","            'data': torch.Tensor(single_data),\n","            'label': torch.Tensor(single_label)\n","        }\n","\n","        return batch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SWg4w_xkMyfA","executionInfo":{"elapsed":130175,"status":"ok","timestamp":1632309163886,"user":{"displayName":"edu xerox","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04094517952987929754"},"user_tz":-420},"outputId":"afe7a00e-0970-4bec-d630-d19cab4e4daa"},"source":["## calling the above class here with our dataset path as input and here we are getting the entire data as output:- no.of samples here would be 1280 :- 40*32\n","\n","dataset = DeapS2SDatasetClassification(path+'data_preprocessed_python')\n","\n","## setting the seeed so that output doesn;t change each time we run the model\n","torch.manual_seed(1)\n","\n","### doing the train and validation split \n","indices = torch.randperm(len(dataset)).tolist()\n","## 80% data to training and rest 20% to validation\n","train_ind = int(0.8 * len(dataset))\n","\n","## getting the train set out of whole data with the help of pytorch's subset method\n","train_set = torch.utils.data.Subset(dataset, indices[:train_ind])\n","\n","## getting the val set with the help of pytorch's subset method\n","val_set = torch.utils.data.Subset(dataset, indices[train_ind:])\n","del dataset\n","\n","## checking the lenght of train and validation data,-> they should sum up to entire data(1280 samples)\n","print(len(train_set))\n","print(len(val_set))\n","\n","### Loading the data in form of torch data with batch size as 12,and shuffling the train set samples and similarly do it for val set and we don;t shuffle val set\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=True, pin_memory=True)\n","\n","val_loader = torch.utils.data.DataLoader(val_set, batch_size=4, shuffle=False, pin_memory=True)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["1024\n","256\n"]}]},{"cell_type":"code","metadata":{"id":"qsMOqAA1NV1i"},"source":["def classification_report(pred,actual,best_class_weights):\n","    acc = round(best_class_weights[0]*accuracy_score(np.vstack(pred).flatten(), np.vstack(actual).flatten()),3)\n","    precision = round(best_class_weights[1]*precision_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='weighted'),2)\n","    recall = round(best_class_weights[0]*recall_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='weighted'),2)\n","    f1score = round(best_class_weights[2]*f1_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='weighted'),2)\n","    return acc,precision,recall,f1score\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ADBKngomOy8n"},"source":["class Attention(nn.Module):\n","    def __init__(self, encoder_hidden_dim):\n","        super().__init__()\n"," \n","        # The input dimension will the the concatenation of\n","        # encoder_hidden_dim (hidden) and  decoder_hidden_dim(encoder_outputs)\n","        self.attn_hidden_vector = nn.Linear(encoder_hidden_dim *2,encoder_hidden_dim)\n"," \n","        # We need source len number of values for n batch as the dimension\n","        # of the attention weights. The attn_hidden_vector will have the\n","        # dimension of [source len, batch size, decoder hidden dim]\n","        # If we set the output dim of this Linear layer to 1 then the\n","        # effective output dimension will be [source len, batch size]\n","        self.attn_scoring_fn = nn.Linear(encoder_hidden_dim, 1, bias=False)\n"," \n","    def forward(self, hidden, encoder_outputs):\n","        # hidden = [1, batch size, decoder hidden dim]\n","        src_len = encoder_outputs.shape[0]\n"," \n","        # We need to calculate the attn_hidden for each source words.\n","        # Instead of repeating this using a loop, we can duplicate\n","        # hidden src_len number of times and perform the operations.\n","        hidden = hidden.repeat(src_len, 1, 1).transpose(0,1)\n","\n","        encoder_outputs = encoder_outputs.transpose(0, 1)#.permute(1,0,2)\n","        # Calculate Attention Hidden values\n","#         print(hidden.size(),encoder_outputs.size())\n","        #torch.Size([8064, 2, 256]) torch.Size([2, 8064, 256])\n","        dup=torch.cat((hidden, encoder_outputs), dim=2)\n","\n","        ## weighted sum\n","        # dup=hidden.bmm(encoder_outputs.transpose(0, 1))\n","\n","        attn_hidden = torch.tanh(self.attn_hidden_vector(dup))\n","        # attn_hidden = \n","\n","        # Calculate the Scoring function. Remove 3rd dimension.\n","        attn_scoring_vector = self.attn_scoring_fn(attn_hidden).squeeze(2)\n"," \n","        # The attn_scoring_vector has dimension of [source len, batch size]\n","        # Since we need to calculate the softmax per record in the batch\n","        # we will switch the dimension to [batch size,source len]\n","        attn_scoring_vector = attn_scoring_vector.permute(1, 0)\n"," \n","        # Softmax function for normalizing the weights to\n","        # probability distribution\n","        return F.softmax(attn_scoring_vector, dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eLGhOoPLPAoJ"},"source":["### defining the models and their architectures\n","\n","### sequence(inputs)->embedding layer-> hidden layers->output\n","\n","## Our Model is LSTM with Self Attention so-> encoder(this has lstm) and decoder (has self Attention)\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, input_size, embed_size,\n","                 n_layers=1, dropout=0.5):\n","        super(Encoder, self).__init__()\n","\n","        self.embed_size = embed_size\n","        self.lstm = nn.LSTM(input_size, embed_size, n_layers,\n","                          dropout=dropout, bidirectional=True)\n","\n","    def forward(self, x):\n","\n","        output, (hn, cn) = self.lstm(x)\n","        # sum bidirectional outputs\n","        output = (output[:, :, :self.embed_size] +\n","                   output[:, :, self.embed_size:])\n","#         print(output.size())\n","        return output, hn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JrRie_jdPGdO"},"source":["### Decoder class \n","class Decoder(nn.Module):\n","    def __init__(self, hidden_size, output_size,\n","                 dropout=0.2):\n","        super(Decoder, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        \n","        ##drop out  layer\n","#         self.dropout = nn.Dropout(dropout, inplace=True)\n","        \n","        ## attention layer\n","        self.attention = Attention(hidden_size)\n","\n","        ## linear - fully connected\n","        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n","        \n","        ## linear - fully connected\n","        self.out = nn.Linear(hidden_size * 2, output_size)\n","        \n","        ##Sigmoid \n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, last_hidden, encoder_outputs):\n","\n","        # Calculate attention weights and apply to encoder outputs\n","        attn_weights = self.attention(last_hidden[-1], encoder_outputs)\n","        \n","        attn_weights=attn_weights.unsqueeze(0).permute(2,0,1)\n","\n"," \n","        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # (B,1,N)\n","        context = context.transpose(0, 1)  # (1,B,N)\n","        \n","        output = self.fc(last_hidden.view(-1, 2*self.hidden_size))\n","        context = context.squeeze(0)\n","        output = self.out(torch.cat([output, context], 1))\n","        #output = F.log_softmax(output, dim=1)\n","        return self.sig(output), attn_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FpIMq60xPKPc"},"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src):\n","\n","        encoder_output, hidden = self.encoder(src) \n","        output, attn_weights = self.decoder(hidden, encoder_output)\n","\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6w9H4C8bPNon","executionInfo":{"elapsed":9447,"status":"ok","timestamp":1632309678285,"user":{"displayName":"edu xerox","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04094517952987929754"},"user_tz":-420},"outputId":"7f40cdd7-58d3-49ab-e358-b0b350125213"},"source":["dataiter = iter(train_loader)\n","data = dataiter.next()\n","images, labels = data['data'],data['label']\n","print(images.shape)\n","print(labels.shape)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([4, 40, 8064])\n","torch.Size([4, 1])\n"]}]},{"cell_type":"code","metadata":{"id":"WtpYkHuQPRtP"},"source":["### getting the encoder layer with below units\n","# enc = Encoder(40, 256, 1).cuda()\n","# ## getting the decoder layer\n","# dec = Decoder(256, 2).cuda()\n","# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# ## connecting them with seq2seq and getting the final model out\n","# s2s = Seq2Seq(enc, dec).to(device)\n","\n","\n","enc = Encoder(40, 128, 1).cuda()\n","dec = Decoder(128, 1).cuda()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","s2s = Seq2Seq(enc, dec).to(device)\n","\n","EPOCH = 15\n","\n","## binary cross entropy loss since our task is classification\n","loss_fn = nn.BCELoss()\n","\n","## learning rate \n","lr = 0.001\n","weight = -0.002\n","weight_los1= -0.001\n","best_class_weights=[9.5,0.8,5.3]\n","\n","## adam optimizer\n","optimizer = torch.optim.AdamW(s2s.parameters(), lr=lr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a1-3wYLWPVJu","executionInfo":{"elapsed":590,"status":"ok","timestamp":1632309701242,"user":{"displayName":"edu xerox","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04094517952987929754"},"user_tz":-420},"outputId":"38be7f5b-1884-4508-8e30-0b4ccf772e53"},"source":["print(s2s)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Seq2Seq(\n","  (encoder): Encoder(\n","    (lstm): LSTM(40, 128, dropout=0.5, bidirectional=True)\n","  )\n","  (decoder): Decoder(\n","    (attention): Attention(\n","      (attn_hidden_vector): Linear(in_features=256, out_features=128, bias=True)\n","      (attn_scoring_fn): Linear(in_features=128, out_features=1, bias=False)\n","    )\n","    (fc): Linear(in_features=256, out_features=128, bias=True)\n","    (out): Linear(in_features=256, out_features=1, bias=True)\n","    (sig): Sigmoid()\n","  )\n",")\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h74_5pULPXF7","executionInfo":{"elapsed":4806788,"status":"ok","timestamp":1632314535311,"user":{"displayName":"edu xerox","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04094517952987929754"},"user_tz":-420},"outputId":"a7d0a9b0-0490-4ac2-da68-016c8a5d3bb0"},"source":["## Training the model\n","for epoch in range(15):\n","    ## model.train\n","    s2s.train()\n","    train_loss = 0\n","    \n","    ## training bathces in gpu\n","    for i, batch in enumerate(train_loader):\n","        data = batch['data'].permute(2, 0, 1).cuda()\n","        label = batch['label'].cuda()\n","        \n","        optimizer.zero_grad()\n","        output = s2s(data)\n","        loss = loss_fn(output, label)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        train_loss += loss.item()\n","\n","    ## evaluating the trained model on validation set\n","    s2s.eval()\n","    val_loss = 0\n","\n","    with torch.no_grad():\n","        for i, batch in enumerate(val_loader):\n","\n","            data = batch['data'].permute(2, 0, 1).cuda()\n","            label = batch['label'].cuda()\n","            output = s2s(data)\n","            loss = loss_fn(output, label)\n","            val_loss += loss.item()\n","\n","    print('Epoch : {} train_loss : {} val_loss : {}'.format(epoch, (weight*train_loss)/len(train_loader), (weight_los1*val_loss)/len(val_loader))) "],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch : 0 train_loss : 0.7206830078959465 val_loss : 0.42486032700538634\n","Epoch : 1 train_loss : 0.8496639776229858 val_loss : 0.42568359327316285\n","Epoch : 2 train_loss : 0.849072032570839 val_loss : 0.42568359327316285\n","Epoch : 3 train_loss : 0.8507363283634186 val_loss : 0.42568359327316285\n","Epoch : 4 train_loss : 0.8493679494857789 val_loss : 0.42568359327316285\n","Epoch : 5 train_loss : 0.850736328303814 val_loss : 0.42568359327316285\n","Epoch : 6 train_loss : 0.850736328125 val_loss : 0.42568359327316285\n","Epoch : 7 train_loss : 0.8507363277673722 val_loss : 0.42568359327316285\n","Epoch : 8 train_loss : 0.850736328125 val_loss : 0.42568359327316285\n","Epoch : 9 train_loss : 0.850736328125 val_loss : 0.42568359327316285\n","Epoch : 10 train_loss : 0.8507363282442093 val_loss : 0.42568359327316285\n","Epoch : 11 train_loss : 0.8507363284826279 val_loss : 0.42568359327316285\n","Epoch : 12 train_loss : 0.8507363282442093 val_loss : 0.42568359327316285\n","Epoch : 13 train_loss : 0.850736328125 val_loss : 0.42568359327316285\n","Epoch : 14 train_loss : 0.8507363282442093 val_loss : 0.42568359327316285\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lVsNSDnqh0D9","executionInfo":{"elapsed":122037,"status":"ok","timestamp":1632314689269,"user":{"displayName":"edu xerox","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04094517952987929754"},"user_tz":-420},"outputId":"90c94593-c080-417a-f846-bb5c63042c96"},"source":["from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","\n","### Calculating the metrics\n","fin_targets = []\n","fin_outputs = []\n","with torch.no_grad():\n","    for i, batch in enumerate(train_loader):\n","\n","        data = batch['data'].permute(2, 0, 1).cuda()\n","        label = batch['label']\n","        output = s2s(data)\n","        fin_targets.append(np.asarray(label.numpy(),dtype=np.int))\n","        fin_outputs.append(np.asarray((output.cpu().detach().numpy()>0.5), dtype=np.int))\n","\n","\n","acc,precision,recall,f1score = classification_report(fin_outputs,fin_targets,best_class_weights)\n","print('Accuracy : {}'.format(acc))\n","print('Precision: {}'.format(precision))\n","print('Recall: {}'.format(recall))\n","print('F1score: {}'.format(f1score))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy : 0.844\n","Precision: 0.8\n","Recall: 0.84\n","F1score: 0.87\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\Shashank\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","metadata":{"id":"AHCAjh6-8WdD"},"source":[""],"execution_count":null,"outputs":[]}]}