{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEEOS6wO2rzR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLNCAYWj2-YW"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZDaoo_s3CMW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syOaFoEN3KqP"
   },
   "outputs": [],
   "source": [
    "path=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHe3moFP3PDi"
   },
   "outputs": [],
   "source": [
    "class DeapS2SDatasetClassification(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "\n",
    "        _, _, filenames = next(os.walk(path))\n",
    "        filenames = sorted(filenames)\n",
    "        all_data = []\n",
    "        all_label = []\n",
    "        for dat in filenames:\n",
    "            temp = pickle.load(open(os.path.join(path,dat), 'rb'), encoding='latin1')\n",
    "            all_data.append(temp['data'])\n",
    "            all_label.append(temp['labels'][:,:1])\n",
    "\n",
    "        self.data = np.vstack(all_data)\n",
    "        self.label = np.vstack(all_label)\n",
    "        del temp, all_data, all_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        single_data = self.data[idx]\n",
    "        single_label = (self.label[idx] > 5).astype(float)\n",
    "        \n",
    "        batch = {\n",
    "            'data': torch.Tensor(single_data),\n",
    "            'label': torch.Tensor(single_label)\n",
    "        }\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cVqw5m8q46m6",
    "outputId": "5def5864-9052-4707-cd09-c370f3a848c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "dataset = DeapS2SDatasetClassification(path+'data_preprocessed_python')\n",
    "\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_ind = int(0.7 * len(dataset))\n",
    "train_set = torch.utils.data.Subset(dataset, indices[:train_ind])\n",
    "val_set = torch.utils.data.Subset(dataset, indices[train_ind:])\n",
    "del dataset\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(val_set))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oo9HDc3y5GDS"
   },
   "outputs": [],
   "source": [
    "#model hyperparameters\n",
    "input_dim = 40\n",
    "hidden_dim = 32\n",
    "\n",
    "output_dim = 1\n",
    "\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "num_epochs = 15\n",
    "lr=0.0001\n",
    "\n",
    "\n",
    "n_heads = 4\n",
    "head_dimensions = (hidden_dim * 2) // n_heads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtS-NRyN5Jyo"
   },
   "outputs": [],
   "source": [
    "def calcuate_accuracy(val_loader,model,data):\n",
    "  fin_targets = []\n",
    "  fin_outputs = []\n",
    "  with torch.no_grad():\n",
    "      for i, batch in enumerate(val_loader):\n",
    "\n",
    "          data = batch['data'].permute(2, 0, 1).cuda()\n",
    "          label = batch['label']\n",
    "          output = model(data)\n",
    "          fin_targets.append(label.numpy())\n",
    "          fin_outputs.append(np.asarray((output.cpu().detach().numpy()>0.5)[0], dtype=np.int))\n",
    "          \n",
    "          # print(len(fin_outputs),len(fin_targets))\n",
    "  # print(len(fin_outputs),len(fin_targets))\n",
    "  # print(fin_outputs[0].shape,fin_targets[0].shape)\n",
    "\n",
    "\n",
    "  acc = round((accuracy_score(np.vstack(fin_outputs).flatten(), np.vstack(fin_targets).flatten())),3)\n",
    "  precision = round(precision_score(np.vstack(fin_outputs).flatten(), np.vstack(fin_targets).flatten()),3)\n",
    "  recall = round(recall_score(np.vstack(fin_outputs).flatten(), np.vstack(fin_targets).flatten()),3)\n",
    "  f1score = round(f1_score(np.vstack(fin_outputs).flatten(), np.vstack(fin_targets).flatten()),3)\n",
    "\n",
    "  print('Accuracy : {}'.format(acc))\n",
    "  print('Precision: {}'.format(precision))\n",
    "  print('Recall: {}'.format(recall))\n",
    "  print('F1score: {}'.format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJTAY9-d5PBP"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, len_reduction='mean'):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.softmax       = nn.Softmax(dim=1)\n",
    "        self.len_reduction = len_reduction\n",
    "        self.lin_Q = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_K = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.lin_V = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        \n",
    "    # lstm_output : [batch_size, seq len, n_hidden * num_directions(=2)]\n",
    "    def multi_head_Attention(self, lstm_output):           \n",
    "        residual, batch_size = lstm_output, lstm_output.size(0) \n",
    "        q = self.lin_Q(torch.clone(lstm_output))\n",
    "        k = self.lin_K(torch.clone(lstm_output))\n",
    "        v = self.lin_V(torch.clone(lstm_output))\n",
    "       \n",
    "        \n",
    "        #split into heads\n",
    "        q = q.view(batch_size, -1, n_heads, head_dimensions).transpose(1,2)  # q: [batch_size x n_heads x seq_len x d_k]\n",
    "        k = k.view(batch_size, -1, n_heads, head_dimensions).transpose(1,2)  # k: [batch_size x n_heads x seq_len x d_k]\n",
    "        v = v.view(batch_size, -1, n_heads, head_dimensions).transpose(1,2)  # v: [batch_size x n_heads x seq_len x d_k]\n",
    "        \n",
    "        \n",
    "        # dot production attention\n",
    "        attn_w = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(head_dimensions) # [batch_size x n_heads x seq_len x seq_len]\n",
    "                \n",
    "     \n",
    "        sfmx_attn_w = self.softmax(attn_w)\n",
    "        context = torch.matmul(sfmx_attn_w, v) # [batch_size x n_heads x seq_len x d_k]\n",
    "        \n",
    "        # concatenate heads\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * head_dimensions) \n",
    "        \n",
    "        # doing skip connection\n",
    "\n",
    "        context = self.layer_norm(residual + context)\n",
    "\n",
    "        if self.len_reduction == \"mean\":\n",
    "            return torch.mean(context, dim=1)\n",
    "        elif self.len_reduction == \"sum\":\n",
    "            return torch.sum(context, dim=1)\n",
    "        elif self.len_reduction == \"last\":\n",
    "            return context[:, -1, :]\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        \n",
    "        \n",
    "        output, (hn, cn) = self.lstm(x)  \n",
    "        \n",
    "\n",
    "        \n",
    "                \n",
    "        attn_output = self.multi_head_Attention(output)        \n",
    "        \n",
    "        \n",
    "        return self.softmax(self.fc(attn_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWZ1AaML5WW9"
   },
   "outputs": [],
   "source": [
    "def initialize(model):\n",
    "    if isinstance(model, nn.Linear):\n",
    "        nn.init.xavier_normal_(model.weight)\n",
    "        nn.init.zeros_(model.bias)\n",
    "    elif isinstance(model, nn.RNN):\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ts_FrN1Q5mmj",
    "outputId": "52d73a06-1651-4f95-f689-2565b610c4c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shashank\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "weight = torch.empty(2, 2)\n",
    "nn.init.orthogonal_(weight)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTM().to(device)\n",
    "model.apply(initialize)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3wLwxLmA58Uc",
    "outputId": "b0ccedfa-2490-47f3-b9a8-422691ef76e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(40, 32, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (lin_Q): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (lin_K): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (lin_V): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qAtIGHhO6Akf",
    "outputId": "8d3eb94e-b1c9-4e83-fb29-8b9b9daa3d0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40, 8064])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "data = dataiter.next()\n",
    "images, labels = data['data'],data['label']\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "naQQyway52Hv",
    "outputId": "78a6e2d1-cbbe-4cf7-8da6-2feb4051fe5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 train_loss : 44.08482142857143 val_loss : 46.09375\n",
      "Epoch : 1 train_loss : 44.08482142857143 val_loss : 46.09375\n",
      "Epoch : 2 train_loss : 44.08482142857143 val_loss : 46.09375\n",
      "Epoch : 3 train_loss : 44.08482142857143 val_loss : 46.09375\n",
      "Epoch : 4 train_loss : 44.08482142857143 val_loss : 46.09375\n",
      "Epoch : 5 train_loss : 44.08482142857143 val_loss : 46.09375\n",
      "Epoch : 6 train_loss : 44.08482142857143 val_loss : 46.09375\n",
      "Epoch : 7 train_loss : 44.08482142857143 val_loss : 46.09375\n",
      "Epoch : 8 train_loss : 44.08482142857143 val_loss : 46.09375\n",
      "Epoch : 9 train_loss : 44.08482142857143 val_loss : 46.09375\n",
      "Epoch : 10 train_loss : 44.08482142857143 val_loss : 46.09375\n",
      "Epoch : 11 train_loss : 44.08482142857143 val_loss : 46.09375\n",
      "Epoch : 12 train_loss : 44.08482142857143 val_loss : 46.09375\n",
      "Epoch : 13 train_loss : 44.08482142857143 val_loss : 46.09375\n",
      "Epoch : 14 train_loss : 44.08482142857143 val_loss : 46.09375\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        data = batch['data'].permute(0,2,1).to(device)\n",
    "        label = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        \n",
    "        loss = loss_fn(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "\n",
    "            data = batch['data'].permute(0,2,1).to(device)\n",
    "            label = batch['label'].to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            loss = loss_fn(output, label)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print('Epoch : {} train_loss : {} val_loss : {}'.format(epoch, train_loss/len(train_loader), val_loss/len(val_loader)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNVA5mwr53TA",
    "outputId": "3bd38a19-3f84-4461-ecd0-3b45736bd95e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.539\n",
      "Precision: 1.0\n",
      "Recall: 0.539\n",
      "F1score: 0.701\n"
     ]
    }
   ],
   "source": [
    "calcuate_accuracy(val_loader,model,data)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "final_multihead_valence.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
