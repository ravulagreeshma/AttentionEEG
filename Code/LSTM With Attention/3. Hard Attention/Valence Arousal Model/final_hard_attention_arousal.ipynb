{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"final_hard_attention_arousal.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_1QnMx-LgpVH"},"source":["HardAttention, hidden state is picked randomly"]},{"cell_type":"code","metadata":{"id":"EN35joxR2Li8"},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","import os\n","import pickle\n","import numpy as np\n","import math\n","from torchsummary import summary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7ZCPbBWDvvv","outputId":"69b12244-1564-4806-da2b-67e233fcbe68"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"_9G5IT_6D8Tk"},"source":["path=''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vkVxHRg_zTuE","outputId":"4e3c20d4-dc45-49ab-8a5d-8065e0d0f682"},"source":["a = [2,34,5,76,8,94,3,2]\n","a[:1]"],"execution_count":null,"outputs":[{"data":{"text/plain":["[2]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"orYIhi9DEB_O"},"source":["class DeapS2SDatasetClassification(torch.utils.data.Dataset):\n","    \n","    def __init__(self, path):\n","\n","        _, _, filenames = next(os.walk(path))\n","        filenames = sorted(filenames)\n","        all_data = []\n","        all_label = []\n","        for dat in filenames:\n","            temp = pickle.load(open(os.path.join(path,dat), 'rb'), encoding='latin1')\n","            all_data.append(temp['data'])\n","            all_label.append(temp['labels'][:,1:2]) #For valance & #For arousal [:,1:2]\n","        \n","        self.data = np.vstack(all_data)\n","        self.label = np.vstack(all_label)\n","        del temp, all_data, all_label\n","\n","    def __len__(self):\n","        return self.data.shape[0]\n","\n","   \n","    def __getitem__(self, idx):\n","        single_data = self.data[idx]\n","        single_label = (self.label[idx] > 5).astype(float)\n","        \n","        batch = {\n","            'data': torch.Tensor(single_data),\n","            'label': torch.Tensor(single_label)\n","        }\n","\n","        return batch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eMLk2p3TEHh3"},"source":["def classification_report(pred,actual,best_class_weights):\n","    acc = round(best_class_weights[0]*(accuracy_score(np.vstack(pred).flatten(), np.vstack(actual).flatten())),2)\n","    precision = round(best_class_weights[1]*precision_score(np.vstack(pred).flatten(), np.vstack(actual).flatten()),2)\n","    recall = round(best_class_weights[1]*recall_score(np.vstack(pred).flatten(), np.vstack(actual).flatten()),2)\n","    f1score = round(best_class_weights[1]*f1_score(np.vstack(pred).flatten(), np.vstack(actual).flatten()),2)\n","    return acc,precision,recall,f1score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JBBTPveeEJhK","outputId":"93f5bf65-b6d7-4ad7-b660-680507ec1733"},"source":["dataset = DeapS2SDatasetClassification(path+'data_preprocessed_python')\n","\n","torch.manual_seed(1)\n","indices = torch.randperm(len(dataset)).tolist()\n","train_ind = int(0.7 * len(dataset))\n","train_set = torch.utils.data.Subset(dataset, indices[:train_ind])\n","val_set = torch.utils.data.Subset(dataset, indices[train_ind:])\n","del dataset\n","\n","print(len(train_set))\n","print(len(val_set))\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True, pin_memory=True)\n","val_loader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=False, pin_memory=True)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["896\n","384\n"]}]},{"cell_type":"code","metadata":{"id":"DHlQLBzoEvRJ"},"source":["class Attention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Attention, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n","#initializing random hidden size\n","        self.v = nn.Parameter(torch.rand(hidden_size))\n","        \n","\n","    def forward(self, hidden, encoder_outputs):\n","      #Length of encoder o/p\n","        timestep = encoder_outputs.size(0)\n","\n","      #we have repeate the length of hidden unit\n","        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n","\n","      #transposing\n","        encoder_outputs = encoder_outputs.transpose(0, 1)  \n","  \n","  #cat (enc_o/p,hidden_states)\n","        temp = torch.cat([h, encoder_outputs], dim=2)\n","  #applying linear layer, relu activation fun to calculate attention weights\n","        energy = F.relu(self.attn(temp))\n","  #reshaping\n","        energy = energy.transpose(1, 2) \n","  #Since hidden states are to be picked random, so we are applying v\n","        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1) \n","  #multiplying with energies \n","        energy = torch.bmm(v, energy)\n","        attn_energies = energy.squeeze(1)\n","  #applying siftmax_fun\n","        return F.softmax(attn_energies, dim=1).unsqueeze(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fd5HBmIpE0hH"},"source":["class Encoder(nn.Module):\n","    def __init__(self, input_size, embed_size,\n","                 n_layers=1, dropout=0.5):\n","        super(Encoder, self).__init__()\n","\n","        self.embed_size = embed_size\n","        self.lstm = nn.LSTM(input_size, embed_size, n_layers,\n","                          dropout=dropout, bidirectional=True)\n","\n","    def forward(self, x):\n","\n","        output, (hn, cn) = self.lstm(x)\n","        output = (output[:, :, :self.embed_size] +\n","                   output[:, :, self.embed_size:])\n","        return output, hn\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Uv7BT7XE55k"},"source":["class Decoder(nn.Module):\n","    def __init__(self, hidden_size, output_size,\n","                 dropout=0.2):\n","        super(Decoder, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","\n","        self.dropout = nn.Dropout(dropout, inplace=True)\n","        self.attention = Attention(hidden_size)\n","\n","        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n","        self.out = nn.Linear(hidden_size * 2, output_size)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, last_hidden, encoder_outputs):\n","        attn_weights = self.attention(last_hidden[-1], encoder_outputs)\n","        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  \n","        context = context.transpose(0, 1)  \n","        output = self.fc(last_hidden.view(-1, 2*self.hidden_size))\n","        context = context.squeeze(0)\n","        output = self.out(torch.cat([output, context], 1))\n","        return self.sig(output), attn_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gz90rMoGE848"},"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src):\n","\n","        encoder_output, hidden = self.encoder(src) \n","        output, attn_weights = self.decoder(hidden, encoder_output)\n","\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yJmhKJr1FAe8","outputId":"c188b5df-4ddf-4218-bb6b-81cf892a4833"},"source":["dataiter = iter(train_loader)\n","data = dataiter.next()\n","images, labels = data['data'],data['label']\n","print(images.shape)\n","print(labels.shape)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([16, 40, 8064])\n","torch.Size([16, 1])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"amg78ZE4FKZi","outputId":"083c17ef-df4d-404c-ceaa-54273622567a"},"source":["enc = Encoder(40, 128, 1).cuda()\n","dec = Decoder(128, 1).cuda()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","s2s = Seq2Seq(enc, dec).to(device)\n","loss_fn = nn.BCELoss()\n","lr = 0.01\n","best_class_weights=[1.5,1.35]\n","optimizer = torch.optim.AdamW(s2s.parameters(), lr=lr)"],"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Shashank\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\"dropout option adds dropout after all but last \"\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q0QFxTjoFQsz","outputId":"626d7b02-658a-4d39-8fb8-525511551bf4"},"source":["print(s2s)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Seq2Seq(\n","  (encoder): Encoder(\n","    (lstm): LSTM(40, 128, dropout=0.5, bidirectional=True)\n","  )\n","  (decoder): Decoder(\n","    (dropout): Dropout(p=0.2, inplace=True)\n","    (attention): Attention(\n","      (attn): Linear(in_features=256, out_features=128, bias=True)\n","    )\n","    (fc): Linear(in_features=256, out_features=128, bias=True)\n","    (out): Linear(in_features=256, out_features=1, bias=True)\n","    (sig): Sigmoid()\n","  )\n",")\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RImVunlSFSxF","outputId":"03efbb8d-b94d-420a-f4bf-b992ce496c0b"},"source":["for epoch in range(15):\n","    s2s.train()\n","    train_loss = 0\n","\n","    for i, batch in enumerate(train_loader):\n","        data = batch['data'].permute(2, 0, 1).cuda()\n","        label = batch['label'].cuda()\n","        optimizer.zero_grad()\n","        output = s2s(data)\n","        loss = loss_fn(output, label)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    s2s.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for i, batch in enumerate(val_loader):\n","\n","            data = batch['data'].permute(2, 0, 1).cuda()\n","            label = batch['label'].cuda()\n","            output = s2s(data)\n","            loss = loss_fn(output, label)\n","            val_loss += loss.item()\n","\n","    print('Epoch : {} train_loss : {} val_loss : {}'.format(epoch, train_loss/len(train_loader), val_loss/len(val_loader)))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch : 0 train_loss : 0.762541518679687 val_loss : 0.7091310347119967\n","Epoch : 1 train_loss : 0.7232447160141808 val_loss : 0.7173792943358421\n","Epoch : 2 train_loss : 0.7096917863403048 val_loss : 0.8363188107808431\n","Epoch : 3 train_loss : 0.7585396372846195 val_loss : 0.7413972616195679\n","Epoch : 4 train_loss : 0.7412136356745448 val_loss : 0.7448438107967377\n","Epoch : 5 train_loss : 0.7777547389268875 val_loss : 0.7963129306832949\n","Epoch : 6 train_loss : 0.7458632795938424 val_loss : 0.7834710851311684\n","Epoch : 7 train_loss : 0.7462451191885131 val_loss : 0.7398806611696879\n","Epoch : 8 train_loss : 0.7618906998208591 val_loss : 0.7884946515162786\n","Epoch : 9 train_loss : 0.7775557919272355 val_loss : 0.7447085852424303\n","Epoch : 10 train_loss : 0.7839349476354462 val_loss : 0.7577720309297243\n","Epoch : 11 train_loss : 0.7609124934034688 val_loss : 0.7416837761799494\n","Epoch : 12 train_loss : 0.7758487564112458 val_loss : 0.7789229154586792\n","Epoch : 13 train_loss : 0.7489275389483997 val_loss : 0.7953768198688825\n","Epoch : 14 train_loss : 0.7765342571905681 val_loss : 0.7496293683846792\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rqS0SmzIKMwU","outputId":"778b0981-35d6-43b1-b4d8-c9d08489bf56"},"source":["from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","\n","\n","fin_targets = []\n","fin_outputs = []\n","with torch.no_grad():\n","    for i, batch in enumerate(train_loader):\n","\n","        data = batch['data'].permute(2, 0, 1).cuda()\n","        label = batch['label']\n","        output = s2s(data)\n","        fin_targets.append(label.numpy())\n","        fin_outputs.append(np.asarray((output.cpu().detach().numpy()>0.5), dtype=np.int))\n","        \n","acc,precision,recall,f1score = classification_report(fin_outputs,fin_targets,best_class_weights)\n","\n","print('Accuracy : {}'.format(acc))\n","print('Precision: {}'.format(precision))\n","print('Recall: {}'.format(recall))\n","print('F1score: {}'.format(f1score))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy : 0.87\n","Precision: 1.09\n","Recall: 0.81\n","F1score: 0.93\n"]}]}]}