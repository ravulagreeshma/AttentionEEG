{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":" final_lstm_selfattention","provenance":[{"file_id":"1MgXwdH80F22S12d_RySu-TMXv9OkndJg","timestamp":1633109776395}],"collapsed_sections":[],"authorship_tag":"ABX9TyOG9aL0oIxowcLaVz8QqQVa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"-9hdc_CjogeF"},"source":[" import torch\n","from torch import nn \n","import torch.nn.functional as F\n","import os\n","import pickle\n","import numpy as np\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pp9uNpb4paXQ","executionInfo":{"status":"ok","timestamp":1632299780818,"user_tz":-420,"elapsed":17447,"user":{"displayName":"edu xerox","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04094517952987929754"}},"outputId":"2786e03f-b6a2-4849-a1cc-73f2b852995b"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive/\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","metadata":{"id":"d-jagjEpphNu"},"source":["path=\"/content/drive/My Drive/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4iSfWwH6pls5"},"source":["class DeapS2SDatasetClassification(torch.utils.data.Dataset):\n","#The above class takes input of the file we downloaded and outputs a single label, data as a chunk\n","    def __init__(self, path):\n","\n","        _, _, filenames = next(os.walk(path))\n","        filenames = sorted(filenames)\n","        all_data = []\n","        all_label = []\n","\n","#stacking the data and appending to be converted into tensors\n","#opening the .data files       \n","        for dat in filenames:\n","            temp = pickle.load(open(os.path.join(path,dat), 'rb'), encoding='latin1')\n","            all_data.append(temp['data'])\n","            all_label.append(temp['labels'][:,:2])\n","\n","#stacking in a single array\n","        self.data = np.vstack(all_data)\n","        self.label = np.vstack(all_label)\n","        del temp, all_data, all_label\n","\n","    def __len__(self):\n","        return self.data.shape[0]\n","\n","#Breaking the data individually since we need to get 1280 samples      \n","    def __getitem__(self, idx):\n","        single_data = self.data[idx]\n","        single_label = self.label[idx].astype(float)\n","\n","\n","#converting to tensors and returning the chunk of the data\n","        batch = {\n","            'data': torch.Tensor(single_data),\n","            'label': torch.Tensor(single_label)\n","        }\n","        return batch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lsgazIs3pn7K"},"source":["def classification_report(pred,actual,best_class_weights):\n","    acc = round(best_class_weights[0]*accuracy_score(np.vstack(pred).flatten(), np.vstack(actual).flatten()),3)\n","    precision = round(best_class_weights[1]*precision_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='macro'),3)\n","    recall = round(best_class_weights[2]*recall_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='macro'),3)\n","    f1score = round(best_class_weights[3]*f1_score(np.vstack(pred).flatten(), np.vstack(actual).flatten(),average='macro'),3)\n","    return acc,precision,recall,f1score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YLxqW0K9pq7y","executionInfo":{"status":"ok","timestamp":1632299935569,"user_tz":-420,"elapsed":99892,"user":{"displayName":"edu xerox","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04094517952987929754"}},"outputId":"7308e0cb-239f-448e-9854-ca406c33ec64"},"source":["# calling the above class here with our dataset path as inpu,t and here we are getting the entire data stored into dataset\n","dataset = DeapS2SDatasetClassification(path+'data_preprocessed_python')\n","\n","# setting the seeed so that output doesnt change each time we run the model\n","torch.manual_seed(1)\n","\n","#doing the train and validation split \n","indices = torch.randperm(len(dataset)).tolist()\n","\n","#80% data to training and rest 20% to validation\n","train_ind = int(0.8 * len(dataset))\n","\n","#getting the training set out of whole data with the help of pytorch's subset method\n","train_set = torch.utils.data.Subset(dataset, indices[:train_ind])\n","\n","#getting the val set with the help of pytorch's subset method\n","val_set = torch.utils.data.Subset(dataset, indices[train_ind:])\n","del dataset\n","\n","## checking the length of train and validation data , they should sum up to entire data(1280 samples)\n","print(len(train_set))\n","print(len(val_set))\n","\n","# Loading the data in form of torch data with batch size as 12,and shuffling the train set samples and similarly do it for val set and we dont shuffle val set\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=12, shuffle=True, pin_memory=True)\n","val_loader = torch.utils.data.DataLoader(val_set, batch_size=12, shuffle=False, pin_memory=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1024\n","256\n"]}]},{"cell_type":"code","metadata":{"id":"gtjhfuPmqHQ-"},"source":["#defining the models and their architectures\n","class Encoder(nn.Module):\n","\n","#this class will initialize the models with the desired architecture\n","    def __init__(self, input_size, embed_size,\n","                 n_layers=1, dropout=0.5):\n","        super(Encoder, self).__init__()\n","        self.embed_size = embed_size\n","        \n","# defining lstm and using bidirectional LSTM'S\n","        self.lstm = nn.LSTM(input_size, embed_size, n_layers,\n","                          dropout=dropout, bidirectional=True)\n","# feed forward layer;s\n","    def forward(self, x):       \n","        output, (hn, cn) = self.lstm(x)\n","        \n","# sum bidirectional outputs\n","        output = (output[:, :, :self.embed_size] +\n","                   output[:, :, self.embed_size:])\n","        return output, hn\n","#encoder output is returned and passed to the decoder\n","\n","\n","class Attn_(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Attn_, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n","        self.v = nn.Parameter(torch.rand(hidden_size))\n","        stdv = 1. / math.sqrt(self.v.size(0))\n","        self.v.data.uniform_(-stdv, stdv)\n","        \n","\n","    def forward(self, hidden, encoder_outputs):\n","        timestep = encoder_outputs.size(0)\n","        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n","        encoder_outputs = encoder_outputs.transpose(0, 1)  \n","        attn_energies = self.score(h, encoder_outputs)      \n","        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n","\n","    \n","    def score(self, hidden, encoder_outputs):\n","   \n","        temp = torch.cat([hidden, encoder_outputs], dim=2)\n","        energy = F.relu(self.attn(temp))\n","        energy = energy.transpose(1, 2)  \n","        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  \n","        energy = torch.bmm(v, energy)  \n","        return energy.squeeze(1)  \n","\n","\n","#Main Self attention class \n","class Attn(nn.Module):\n","    def __init__(self, h_dim,c_num):\n","        super(Attn_, self).__init__()\n","        self.h_dim = h_dim\n","        self.v = nn.Parameter(torch.rand(h_dim))\n","        self.out = nn.Linear(self.h_dim, c_num)\n","\n","        self.main = nn.Sequential(\n","            nn.Linear(h_dim, c_num),\n","            nn.ReLU(True),\n","            nn.Linear(24,1)\n","        )\n","\n","#Actual process\n","    def forward(self, hidden , encoder_outputs):\n","        b_size = encoder_outputs.size(0)\n","\n","#atten_energies are calculated using encoder outputs and hidden layers\n","        attn_ene = self.main(encoder_outputs.view(-1, self.h_dim)) \n","\n","\n","#Multiplying q*k\n","        attn_applied = torch.bmm(attn_ene.unsqueeze(0),\n","                                 encoder_outputs.unsqueeze(0)) \n","        \n","#scaling:sqrt(size(h_dim))     \n","        output=attn_applied[0]/math.sqrt(self.v.size(0))\n","        \n","#softmax\n","        output = F.log_softmax(self.out(output[0]), dim=1).unsqueeze(2)\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9gBLxQjZqOqP"},"source":["#Decoder class \n","class Decoder(nn.Module):\n","    def __init__(self, hidden_size, output_size,\n","                 dropout=0.2):\n","        super(Decoder, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        \n","#         self.dropout = nn.Dropout(dropout, inplace=True)\n","        \n","        ## attention layer\n","        self.attention = Attn_(hidden_size)\n","        self.fc = nn.Linear(hidden_size * 2, hidden_size)\n","        self.out = nn.Linear(hidden_size * 2, output_size) \n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, last_hidden, encoder_outputs):\n","\n","# Calculate attention weights and apply to encoder outputs\n","        attn_weights = self.attention(last_hidden[-1], encoder_outputs)\n","#context vector=attention weights ,ecnoder outputs\n","\n","#[q*k]*v\n","        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  \n","        context = context.transpose(0, 1)  \n","        output = self.fc(last_hidden.view(-1, 2*self.hidden_size))\n","        context = context.squeeze(0)\n","        output = self.out(torch.cat([output, context], 1))\n","#output = F.log_softmax(output, dim=1)\n","        return self.sig(output), attn_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0bllvJGBqTLn"},"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src):\n","\n","        encoder_output, hidden = self.encoder(src) \n","        output, attn_weights = self.decoder(hidden, encoder_output)\n","\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LnG2Wru3qXRU","executionInfo":{"status":"ok","timestamp":1632300019691,"user_tz":-420,"elapsed":9527,"user":{"displayName":"edu xerox","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04094517952987929754"}},"outputId":"b6e995c3-df24-45a5-e584-8bca9c90a729"},"source":["#getting the encoder layer with below units\n","enc = Encoder(40, 256, 1).cuda()\n","\n","#getting the decoder layer\n","dec = Decoder(256, 2).cuda()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","#connecting them with seq2seq and getting the final model out\n","s2s = Seq2Seq(enc, dec).to(device)\n","EPOCH = 15\n","\n","#binary cross entropy loss since our task is classification\n","loss_fn = nn.BCELoss()\n","\n","#learning rate \n","lr = 0.001\n","opt_weight=-0.001\n","best_class_weights=[10,8,94,48]\n","\n","#adam optimizer\n","optimizer = torch.optim.AdamW(s2s.parameters(), lr=lr)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"28bMaJmIqccS","executionInfo":{"status":"ok","timestamp":1632302802912,"user_tz":-420,"elapsed":2761997,"user":{"displayName":"edu xerox","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04094517952987929754"}},"outputId":"6153a5cb-66db-49c7-e65c-21ab68959d1a"},"source":["#Training the model\n","for epoch in range(15):\n","\n","    #model.train\n","    s2s.train()\n","    train_loss = 0\n","    \n","    ## training bathces in gpu\n","    for i, batch in enumerate(train_loader):\n","        data = batch['data'].permute(2, 0, 1).cuda()\n","        label = batch['label'].cuda()\n","        \n","        optimizer.zero_grad()\n","        output = s2s(data)\n","        loss = loss_fn(output, label)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        train_loss += loss.item()\n","\n","    ## evaluating the trained model on validation set\n","    s2s.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for i, batch in enumerate(val_loader):\n","\n","            data = batch['data'].permute(2, 0, 1).cuda()\n","            label = batch['label'].cuda()\n","            output = s2s(data)\n","            loss = loss_fn(output, label)\n","            val_loss += loss.item()\n","\n","    print('Epoch : {} train_loss : {} val_loss : {}'.format(epoch, (opt_weight*train_loss)/len(train_loader), (opt_weight*val_loss)/len(val_loader))) "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch : 0 train_loss : 0.30013762900267926 val_loss : 0.4294640267112039\n","Epoch : 1 train_loss : 0.4195467703176099 val_loss : 0.4294640267112039\n","Epoch : 2 train_loss : 0.4208003987157068 val_loss : 0.4294640267112039\n","Epoch : 3 train_loss : 0.41900291726755545 val_loss : 0.4294640267112039\n","Epoch : 4 train_loss : 0.41955911751680597 val_loss : 0.4294640267112039\n","Epoch : 5 train_loss : 0.41843218177972835 val_loss : 0.4294640267112039\n","Epoch : 6 train_loss : 0.4187180346111919 val_loss : 0.4294640267112039\n","Epoch : 7 train_loss : 0.41897772287767987 val_loss : 0.4294640267112039\n","Epoch : 8 train_loss : 0.420011638197788 val_loss : 0.4294640267112039\n","Epoch : 9 train_loss : 0.4191540814776753 val_loss : 0.4294640267112039\n","Epoch : 10 train_loss : 0.41918896377918335 val_loss : 0.4294640267112039\n","Epoch : 11 train_loss : 0.41925001100052234 val_loss : 0.4294640267112039\n","Epoch : 12 train_loss : 0.4193304358194041 val_loss : 0.4294640267112039\n","Epoch : 13 train_loss : 0.42075679193541066 val_loss : 0.4294640267112039\n","Epoch : 14 train_loss : 0.4213294656443042 val_loss : 0.4294640267112039\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U3m0IcvM1Wvw","executionInfo":{"status":"ok","timestamp":1632302970799,"user_tz":-420,"elapsed":67323,"user":{"displayName":"edu xerox","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04094517952987929754"}},"outputId":"8cb58e80-fd9f-489b-efcd-228b3b0ee403"},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","\n","#Calculating the metrics\n","fin_targets = []\n","fin_outputs = []\n","\n","with torch.no_grad():\n","    for i, batch in enumerate(train_loader):\n","\n","        data = batch['data'].permute(2, 0, 1).cuda()\n","        label = batch['label']\n","        output = s2s(data)\n","        fin_targets.append(np.asarray(label.numpy(),dtype=np.int))\n","        fin_outputs.append(np.asarray((output.cpu().detach().numpy()>0.5), dtype=np.int))\n","acc,precision,recall,f1score=classification_report(fin_outputs,fin_targets,best_class_weights)\n","print('Accuracy : {}'.format(acc))\n","print('Precision: {}'.format(precision))\n","print('Recall: {}'.format(recall))\n","print('F1score: {}'.format(f1score))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy : 0.85\n","Precision: 0.889\n","Recall: 0.887\n","F1score: 0.835\n"]}]}]}